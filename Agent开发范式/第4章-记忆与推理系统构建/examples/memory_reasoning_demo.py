# memory_reasoning_demo.py
"""
ç¬¬4ç«  è®°å¿†ä¸æ¨ç†ç³»ç»Ÿæ„å»º - æ¼”ç¤ºç¨‹åº
å±•ç¤ºè®°å¿†ç³»ç»Ÿã€çŸ¥è¯†å›¾è°±ã€æ¨ç†å¼•æ“ã€å­¦ä¹ ç³»ç»Ÿå’Œæ£€ç´¢ç³»ç»Ÿçš„ç»¼åˆåº”ç”¨
"""

import asyncio
import logging
import json
import yaml
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import random

# å¯¼å…¥ç¬¬4ç« çš„æ ¸å¿ƒæ¨¡å—
from code.memory_system import MemorySystem, MemoryEntry, MemoryType, MemoryQuery
from code.knowledge_graph import KnowledgeGraph, Entity, Relation, EntityType, RelationType, GraphQuery
from code.reasoning_engine import ReasoningEngine, Rule, Fact, ReasoningType, InferenceMethod, RuleType
from code.learning_system import LearningSystem, LearningTask, TrainingData, LearningType, LearningAlgorithm, LearningMode
from code.retrieval_system import RetrievalSystem, Document, Query, RetrievalType, RetrievalResult

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MemoryReasoningDemo:
    """è®°å¿†ä¸æ¨ç†ç³»ç»Ÿæ¼”ç¤ºç±»"""
    
    def __init__(self, config_path: str = "config/memory_reasoning_configs.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        
        # åˆå§‹åŒ–å„ä¸ªç³»ç»Ÿ
        self.memory_system = MemorySystem(self.config.get("memory_system", {}))
        self.knowledge_graph = KnowledgeGraph(self.config.get("knowledge_graph", {}))
        self.reasoning_engine = ReasoningEngine(self.config.get("reasoning_engine", {}))
        self.learning_system = LearningSystem(self.config.get("learning_system", {}))
        self.retrieval_system = RetrievalSystem(self.config.get("retrieval_system", {}))
        
        self.running = False
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"Configuration loaded from {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            return {}
    
    async def start(self):
        """å¯åŠ¨æ‰€æœ‰ç³»ç»Ÿ"""
        logger.info("Starting Memory and Reasoning Systems...")
        
        await self.memory_system.start()
        await self.knowledge_graph.start()
        await self.reasoning_engine.start()
        await self.learning_system.start()
        await self.retrieval_system.start()
        
        self.running = True
        logger.info("All systems started successfully")
    
    async def stop(self):
        """åœæ­¢æ‰€æœ‰ç³»ç»Ÿ"""
        logger.info("Stopping Memory and Reasoning Systems...")
        
        await self.memory_system.stop()
        await self.knowledge_graph.stop()
        await self.reasoning_engine.stop()
        await self.learning_system.stop()
        await self.retrieval_system.stop()
        
        self.running = False
        logger.info("All systems stopped successfully")
    
    async def demo_memory_system(self):
        """æ¼”ç¤ºè®°å¿†ç³»ç»Ÿ"""
        print("\n" + "="*60)
        print("ğŸ§  è®°å¿†ç³»ç»Ÿæ¼”ç¤º")
        print("="*60)
        
        # æ·»åŠ ä¸åŒç±»å‹çš„è®°å¿†
        memories = [
            MemoryEntry(
                content="ä»Šå¤©å­¦ä¹ äº†Pythonç¼–ç¨‹",
                memory_type=MemoryType.EPISODIC,
                importance=0.8,
                context={"date": "2024-01-15", "topic": "ç¼–ç¨‹"}
            ),
            MemoryEntry(
                content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€",
                memory_type=MemoryType.SEMANTIC,
                importance=0.9,
                context={"category": "çŸ¥è¯†", "subject": "ç¼–ç¨‹è¯­è¨€"}
            ),
            MemoryEntry(
                content="å¦‚ä½•å®‰è£…PythonåŒ…ï¼špip install package_name",
                memory_type=MemoryType.PROCEDURAL,
                importance=0.7,
                context={"skill": "åŒ…ç®¡ç†", "tool": "pip"}
            ),
            MemoryEntry(
                content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
                memory_type=MemoryType.SEMANTIC,
                importance=0.8,
                context={"field": "AI", "subfield": "ML"}
            ),
            MemoryEntry(
                content="æ˜¨å¤©å®Œæˆäº†æœºå™¨å­¦ä¹ é¡¹ç›®",
                memory_type=MemoryType.EPISODIC,
                importance=0.6,
                context={"date": "2024-01-14", "project": "MLé¡¹ç›®"}
            )
        ]
        
        print("æ·»åŠ è®°å¿†...")
        for memory in memories:
            await self.memory_system.add_memory(memory)
            print(f"âœ“ æ·»åŠ è®°å¿†: {memory.content[:30]}...")
        
        # æ£€ç´¢è®°å¿†
        print("\næ£€ç´¢è®°å¿†:")
        queries = [
            MemoryQuery(query_text="Python", limit=3),
            MemoryQuery(query_text="æœºå™¨å­¦ä¹ ", memory_types=[MemoryType.SEMANTIC], limit=2),
            MemoryQuery(query_text="æ˜¨å¤©", memory_types=[MemoryType.EPISODIC], limit=2)
        ]
        
        for i, query in enumerate(queries, 1):
            print(f"\næŸ¥è¯¢ {i}: {query.query_text}")
            results = await self.memory_system.retrieve_memories(query)
            for j, result in enumerate(results, 1):
                print(f"  {j}. {result.content} (é‡è¦æ€§: {result.importance:.2f})")
        
        # æ›´æ–°è®°å¿†
        print("\næ›´æ–°è®°å¿†:")
        first_memory = memories[0]
        await self.memory_system.update_memory(first_memory.id, "ä»Šå¤©æ·±å…¥å­¦ä¹ äº†Pythoné«˜çº§ç¼–ç¨‹æŠ€å·§")
        print(f"âœ“ æ›´æ–°è®°å¿†: {first_memory.content} -> ä»Šå¤©æ·±å…¥å­¦ä¹ äº†Pythoné«˜çº§ç¼–ç¨‹æŠ€å·§")
        
        # é—å¿˜è®°å¿†
        print("\né—å¿˜è®°å¿†:")
        last_memory = memories[-1]
        await self.memory_system.forget_memory(last_memory.id)
        print(f"âœ“ é—å¿˜è®°å¿†: {last_memory.content}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.memory_system.get_stats()
        print(f"\nè®°å¿†ç³»ç»Ÿç»Ÿè®¡:")
        print(f"  æ€»è®°å¿†æ•°: {stats['total_memories']}")
        print(f"  æƒ…æ™¯è®°å¿†: {stats['memory_type_counts']['episodic']}")
        print(f"  è¯­ä¹‰è®°å¿†: {stats['memory_type_counts']['semantic']}")
        print(f"  ç¨‹åºè®°å¿†: {stats['memory_type_counts']['procedural']}")
    
    async def demo_knowledge_graph(self):
        """æ¼”ç¤ºçŸ¥è¯†å›¾è°±"""
        print("\n" + "="*60)
        print("ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±æ¼”ç¤º")
        print("="*60)
        
        # åˆ›å»ºå®ä½“
        entities = [
            Entity(name="Python", entity_type=EntityType.CONCEPT, attributes={"type": "ç¼–ç¨‹è¯­è¨€", "creator": "Guido van Rossum"}),
            Entity(name="æœºå™¨å­¦ä¹ ", entity_type=EntityType.CONCEPT, attributes={"type": "æŠ€æœ¯", "field": "äººå·¥æ™ºèƒ½"}),
            Entity(name="æ·±åº¦å­¦ä¹ ", entity_type=EntityType.CONCEPT, attributes={"type": "æŠ€æœ¯", "field": "æœºå™¨å­¦ä¹ "}),
            Entity(name="ç¥ç»ç½‘ç»œ", entity_type=EntityType.CONCEPT, attributes={"type": "ç®—æ³•", "field": "æ·±åº¦å­¦ä¹ "}),
            Entity(name="TensorFlow", entity_type=EntityType.OBJECT, attributes={"type": "æ¡†æ¶", "language": "Python"}),
            Entity(name="PyTorch", entity_type=EntityType.OBJECT, attributes={"type": "æ¡†æ¶", "language": "Python"}),
            Entity(name="Guido van Rossum", entity_type=EntityType.PERSON, attributes={"profession": "ç¨‹åºå‘˜", "nationality": "è·å…°"}),
            Entity(name="Google", entity_type=EntityType.ORGANIZATION, attributes={"type": "ç§‘æŠ€å…¬å¸", "founded": "1998"})
        ]
        
        print("æ·»åŠ å®ä½“...")
        for entity in entities:
            await self.knowledge_graph.add_entity(entity)
            print(f"âœ“ æ·»åŠ å®ä½“: {entity.name} ({entity.entity_type.value})")
        
        # åˆ›å»ºå…³ç³»
        relations = [
            Relation(
                source_entity_id=entities[1].id,  # æœºå™¨å­¦ä¹ 
                target_entity_id=entities[2].id,  # æ·±åº¦å­¦ä¹ 
                relation_type=RelationType.PART_OF,
                confidence=0.9
            ),
            Relation(
                source_entity_id=entities[2].id,  # æ·±åº¦å­¦ä¹ 
                target_entity_id=entities[3].id,  # ç¥ç»ç½‘ç»œ
                relation_type=RelationType.PART_OF,
                confidence=0.8
            ),
            Relation(
                source_entity_id=entities[0].id,  # Python
                target_entity_id=entities[4].id,  # TensorFlow
                relation_type=RelationType.RELATED_TO,
                confidence=0.9
            ),
            Relation(
                source_entity_id=entities[0].id,  # Python
                target_entity_id=entities[5].id,  # PyTorch
                relation_type=RelationType.RELATED_TO,
                confidence=0.9
            ),
            Relation(
                source_entity_id=entities[6].id,  # Guido van Rossum
                target_entity_id=entities[0].id,  # Python
                relation_type=RelationType.FOUNDED,
                confidence=1.0
            ),
            Relation(
                source_entity_id=entities[7].id,  # Google
                target_entity_id=entities[4].id,  # TensorFlow
                relation_type=RelationType.FOUNDED,
                confidence=0.9
            )
        ]
        
        print("\næ·»åŠ å…³ç³»...")
        for relation in relations:
            source_name = entities[0].name  # ä¸´æ—¶è·å–åç§°
            target_name = entities[0].name   # ä¸´æ—¶è·å–åç§°
            for entity in entities:
                if entity.id == relation.source_entity_id:
                    source_name = entity.name
                if entity.id == relation.target_entity_id:
                    target_name = entity.name
            
            await self.knowledge_graph.add_relation(relation)
            print(f"âœ“ æ·»åŠ å…³ç³»: {source_name} {relation.relation_type.value} {target_name}")
        
        # å®ä½“æœç´¢
        print("\nå®ä½“æœç´¢:")
        search_queries = ["Python", "å­¦ä¹ ", "ç½‘ç»œ"]
        for query_text in search_queries:
            results = await self.knowledge_graph.find_entity(query_text)
            print(f"æœç´¢ '{query_text}': æ‰¾åˆ° {len(results)} ä¸ªå®ä½“")
            for result in results:
                print(f"  - {result.name} ({result.entity_type.value})")
        
        # å…³ç³»æœç´¢
        print("\nå…³ç³»æœç´¢:")
        python_entity = entities[0]
        relations = await self.knowledge_graph.find_relations(python_entity.id)
        print(f"ä¸ '{python_entity.name}' ç›¸å…³çš„å…³ç³»:")
        for relation in relations:
            source_entity = self.knowledge_graph.entities[relation.source_entity_id]
            target_entity = self.knowledge_graph.entities[relation.target_entity_id]
            print(f"  - {source_entity.name} {relation.relation_type.value} {target_entity.name}")
        
        # è·¯å¾„æŸ¥æ‰¾
        print("\nè·¯å¾„æŸ¥æ‰¾:")
        paths = await self.knowledge_graph.find_path(entities[0].id, entities[3].id)  # Python -> ç¥ç»ç½‘ç»œ
        print(f"ä» '{entities[0].name}' åˆ° '{entities[3].name}' çš„è·¯å¾„:")
        for path in paths:
            print(f"  - {' -> '.join(path)}")
        
        # ä¸­å¿ƒæ€§è®¡ç®—
        print("\nä¸­å¿ƒæ€§è®¡ç®—:")
        centrality = await self.knowledge_graph.calculate_centrality(entities[0].id)  # Python
        print(f"'{entities[0].name}' çš„ä¸­å¿ƒæ€§:")
        for metric, value in centrality.items():
            print(f"  - {metric}: {value:.3f}")
        
        # ç¤¾åŒºå‘ç°
        print("\nç¤¾åŒºå‘ç°:")
        communities = await self.knowledge_graph.find_communities()
        print(f"å‘ç° {len(communities)} ä¸ªç¤¾åŒº:")
        for community_id, entity_ids in communities.items():
            entity_names = [self.knowledge_graph.entities[eid].name for eid in entity_ids if eid in self.knowledge_graph.entities]
            print(f"  - {community_id}: {', '.join(entity_names)}")
        
        # ä»æ–‡æœ¬æå–
        print("\nä»æ–‡æœ¬æå–:")
        text = "Pythonæ˜¯æœºå™¨å­¦ä¹ ä¸­å¸¸ç”¨çš„ç¼–ç¨‹è¯­è¨€ã€‚æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œã€‚TensorFlowæ˜¯Googleå¼€å‘çš„æ¡†æ¶ã€‚"
        extracted_entities, extracted_relations = await self.knowledge_graph.extract_from_text(text)
        print(f"ä»æ–‡æœ¬ä¸­æå–äº† {len(extracted_entities)} ä¸ªå®ä½“å’Œ {len(extracted_relations)} ä¸ªå…³ç³»")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.knowledge_graph.get_stats()
        print(f"\nçŸ¥è¯†å›¾è°±ç»Ÿè®¡:")
        print(f"  æ€»å®ä½“æ•°: {stats['total_entities']}")
        print(f"  æ€»å…³ç³»æ•°: {stats['total_relations']}")
        print(f"  å›¾å¯†åº¦: {stats['density']:.3f}")
        print(f"  æ˜¯å¦è¿é€š: {stats['is_connected']}")
    
    async def demo_reasoning_engine(self):
        """æ¼”ç¤ºæ¨ç†å¼•æ“"""
        print("\n" + "="*60)
        print("ğŸ§© æ¨ç†å¼•æ“æ¼”ç¤º")
        print("="*60)
        
        # æ·»åŠ è§„åˆ™
        rules = [
            Rule(
                name="é¸Ÿç±»è§„åˆ™",
                rule_type=RuleType.IF_THEN,
                antecedent=["æ˜¯é¸Ÿ"],
                consequent=["ä¼šé£"],
                confidence=0.9,
                priority=1
            ),
            Rule(
                name="ä¼é¹…è§„åˆ™",
                rule_type=RuleType.IF_THEN,
                antecedent=["æ˜¯ä¼é¹…"],
                consequent=["æ˜¯é¸Ÿ", "ä¸ä¼šé£"],
                confidence=1.0,
                priority=2
            ),
            Rule(
                name="é£è¡Œè§„åˆ™",
                rule_type=RuleType.IF_THEN,
                antecedent=["ä¼šé£"],
                consequent=["æœ‰ç¿…è†€"],
                confidence=0.8,
                priority=1
            ),
            Rule(
                name="ç¼–ç¨‹è¯­è¨€è§„åˆ™",
                rule_type=RuleType.IF_THEN,
                antecedent=["æ˜¯ç¼–ç¨‹è¯­è¨€"],
                consequent=["å¯ä»¥å†™ç¨‹åº"],
                confidence=0.9,
                priority=1
            )
        ]
        
        print("æ·»åŠ è§„åˆ™...")
        for rule in rules:
            await self.reasoning_engine.add_rule(rule)
            print(f"âœ“ æ·»åŠ è§„åˆ™: {rule.name}")
        
        # æ·»åŠ äº‹å®
        facts = [
            Fact(statement="æ˜¯ä¼é¹…", confidence=1.0, source="è§‚å¯Ÿ"),
            Fact(statement="æ˜¯ç¼–ç¨‹è¯­è¨€", confidence=1.0, source="çŸ¥è¯†"),
            Fact(statement="Pythonæ˜¯ç¼–ç¨‹è¯­è¨€", confidence=1.0, source="çŸ¥è¯†")
        ]
        
        print("\næ·»åŠ äº‹å®...")
        for fact in facts:
            await self.reasoning_engine.add_fact(fact)
            print(f"âœ“ æ·»åŠ äº‹å®: {fact.statement}")
        
        # å‰å‘é“¾æ¥æ¨ç†
        print("\nå‰å‘é“¾æ¥æ¨ç†:")
        forward_results = await self.reasoning_engine.reason(
            ReasoningType.DEDUCTIVE,
            InferenceMethod.FORWARD_CHAINING
        )
        
        for result in forward_results:
            print(f"ç»“è®º: {result.conclusion}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print(f"å‰æ: {', '.join(result.premises)}")
            print(f"è¯æ®: {', '.join(result.evidence)}")
            print()
        
        # åå‘é“¾æ¥æ¨ç†
        print("åå‘é“¾æ¥æ¨ç†:")
        backward_results = await self.reasoning_engine.reason(
            ReasoningType.DEDUCTIVE,
            InferenceMethod.BACKWARD_CHAINING,
            [],
            "ä¼šé£"
        )
        
        for result in backward_results:
            print(f"ç»“è®º: {result.conclusion}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print(f"å‰æ: {', '.join(result.premises)}")
            print(f"è¯æ®: {', '.join(result.evidence)}")
            print()
        
        # å‡è¨€æ¨ç†
        print("å‡è¨€æ¨ç†:")
        modus_ponens_results = await self.reasoning_engine.reason(
            ReasoningType.DEDUCTIVE,
            InferenceMethod.MODUS_PONENS,
            ["æ˜¯ç¼–ç¨‹è¯­è¨€ -> å¯ä»¥å†™ç¨‹åº", "æ˜¯ç¼–ç¨‹è¯­è¨€"]
        )
        
        for result in modus_ponens_results:
            print(f"ç»“è®º: {result.conclusion}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print()
        
        # æ‹’å–å¼
        print("æ‹’å–å¼:")
        modus_tollens_results = await self.reasoning_engine.reason(
            ReasoningType.DEDUCTIVE,
            InferenceMethod.MODUS_TOLLENS,
            ["æ˜¯é¸Ÿ -> ä¼šé£", "ä¸ä¼šé£"]
        )
        
        for result in modus_tollens_results:
            print(f"ç»“è®º: {result.conclusion}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print()
        
        # ä¸‰æ®µè®º
        print("ä¸‰æ®µè®º:")
        syllogism_results = await self.reasoning_engine.reason(
            ReasoningType.DEDUCTIVE,
            InferenceMethod.SYLLOGISM,
            ["æ‰€æœ‰é¸Ÿéƒ½æ˜¯åŠ¨ç‰©", "æ‰€æœ‰åŠ¨ç‰©éƒ½æ˜¯ç”Ÿç‰©"]
        )
        
        for result in syllogism_results:
            print(f"ç»“è®º: {result.conclusion}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print()
        
        # æ¦‚ç‡æ¨ç†
        print("æ¦‚ç‡æ¨ç†:")
        await self.reasoning_engine.add_variable("ä¸‹é›¨", 0.3)
        await self.reasoning_engine.add_variable("å¸¦ä¼", 0.1)
        await self.reasoning_engine.add_dependency("ä¸‹é›¨", "å¸¦ä¼", 0.8)
        await self.reasoning_engine.set_evidence("ä¸‹é›¨", True)
        
        probabilistic_results = await self.reasoning_engine.reason(
            ReasoningType.PROBABILISTIC,
            InferenceMethod.BAYESIAN,
            ["å¸¦ä¼"]
        )
        
        for result in probabilistic_results:
            print(f"ç»“è®º: {result.conclusion}")
            print(f"ç½®ä¿¡åº¦: {result.confidence:.2f}")
            print()
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.reasoning_engine.get_stats()
        print(f"\næ¨ç†å¼•æ“ç»Ÿè®¡:")
        print(f"  æ€»è§„åˆ™æ•°: {stats['total_rules']}")
        print(f"  æ€»äº‹å®æ•°: {stats['total_facts']}")
        print(f"  å·¥ä½œè®°å¿†å¤§å°: {stats['working_memory_size']}")
    
    async def demo_learning_system(self):
        """æ¼”ç¤ºå­¦ä¹ ç³»ç»Ÿ"""
        print("\n" + "="*60)
        print("ğŸ“ å­¦ä¹ ç³»ç»Ÿæ¼”ç¤º")
        print("="*60)
        
        # åˆ›å»ºç›‘ç£å­¦ä¹ ä»»åŠ¡
        print("åˆ›å»ºç›‘ç£å­¦ä¹ ä»»åŠ¡...")
        supervised_task = LearningTask(
            name="æˆ¿ä»·é¢„æµ‹",
            learning_type=LearningType.SUPERVISED,
            algorithm=LearningAlgorithm.LINEAR_REGRESSION,
            mode=LearningMode.BATCH
        )
        
        await self.learning_system.create_task(supervised_task)
        print(f"âœ“ åˆ›å»ºä»»åŠ¡: {supervised_task.name}")
        
        # æ·»åŠ è®­ç»ƒæ•°æ®
        print("\næ·»åŠ è®­ç»ƒæ•°æ®...")
        training_data = [
            TrainingData(features=[100, 3, 2], label=500000),  # é¢ç§¯, æˆ¿é—´æ•°, æµ´å®¤æ•°, ä»·æ ¼
            TrainingData(features=[150, 4, 3], label=750000),
            TrainingData(features=[200, 5, 4], label=1000000),
            TrainingData(features=[120, 3, 2], label=600000),
            TrainingData(features=[180, 4, 3], label=900000),
            TrainingData(features=[250, 6, 5], label=1250000),
            TrainingData(features=[90, 2, 1], label=450000),
            TrainingData(features=[160, 4, 3], label=800000),
            TrainingData(features=[220, 5, 4], label=1100000),
            TrainingData(features=[140, 3, 2], label=700000)
        ]
        
        for data in training_data:
            await self.learning_system.add_training_data(supervised_task.id, data)
        
        print(f"âœ“ æ·»åŠ äº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®")
        
        # è®­ç»ƒæ¨¡å‹
        print("\nè®­ç»ƒæ¨¡å‹...")
        await self.learning_system.train_model(supervised_task.id)
        print("âœ“ æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # é¢„æµ‹
        print("\nè¿›è¡Œé¢„æµ‹...")
        test_cases = [
            [130, 3, 2],  # 130å¹³ç±³, 3æˆ¿é—´, 2æµ´å®¤
            [200, 4, 3],  # 200å¹³ç±³, 4æˆ¿é—´, 3æµ´å®¤
            [80, 2, 1]    # 80å¹³ç±³, 2æˆ¿é—´, 1æµ´å®¤
        ]
        
        for i, features in enumerate(test_cases, 1):
            prediction_result = await self.learning_system.predict(supervised_task.id, features)
            print(f"æµ‹è¯•æ¡ˆä¾‹ {i}: {features} -> é¢„æµ‹ä»·æ ¼: {prediction_result.predictions[0]:.0f}å…ƒ")
            print(f"  å‡†ç¡®ç‡: {prediction_result.accuracy:.3f}")
            print(f"  ç½®ä¿¡åº¦: {prediction_result.confidence:.3f}")
        
        # åˆ›å»ºæ— ç›‘ç£å­¦ä¹ ä»»åŠ¡
        print("\nåˆ›å»ºæ— ç›‘ç£å­¦ä¹ ä»»åŠ¡...")
        unsupervised_task = LearningTask(
            name="å®¢æˆ·èšç±»",
            learning_type=LearningType.UNSUPERVISED,
            algorithm=LearningAlgorithm.K_MEANS,
            mode=LearningMode.BATCH
        )
        
        await self.learning_system.create_task(unsupervised_task)
        print(f"âœ“ åˆ›å»ºä»»åŠ¡: {unsupervised_task.name}")
        
        # æ·»åŠ èšç±»æ•°æ®
        print("\næ·»åŠ èšç±»æ•°æ®...")
        cluster_data = [
            TrainingData(features=[25, 50000, 2]),  # å¹´é¾„, æ”¶å…¥, æ¶ˆè´¹æ¬¡æ•°
            TrainingData(features=[35, 80000, 5]),
            TrainingData(features=[45, 120000, 8]),
            TrainingData(features=[28, 60000, 3]),
            TrainingData(features=[38, 90000, 6]),
            TrainingData(features=[48, 150000, 10]),
            TrainingData(features=[22, 40000, 1]),
            TrainingData(features=[32, 70000, 4]),
            TrainingData(features=[42, 110000, 7]),
            TrainingData(features=[52, 180000, 12])
        ]
        
        for data in cluster_data:
            await self.learning_system.add_training_data(unsupervised_task.id, data)
        
        print(f"âœ“ æ·»åŠ äº† {len(cluster_data)} æ¡èšç±»æ•°æ®")
        
        # è®­ç»ƒèšç±»æ¨¡å‹
        print("\nè®­ç»ƒèšç±»æ¨¡å‹...")
        await self.learning_system.train_model(unsupervised_task.id)
        print("âœ“ èšç±»æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # èšç±»é¢„æµ‹
        print("\nè¿›è¡Œèšç±»é¢„æµ‹...")
        test_customers = [
            [30, 70000, 4],  # 30å², 7ä¸‡æ”¶å…¥, 4æ¬¡æ¶ˆè´¹
            [40, 100000, 6], # 40å², 10ä¸‡æ”¶å…¥, 6æ¬¡æ¶ˆè´¹
            [50, 160000, 9]  # 50å², 16ä¸‡æ”¶å…¥, 9æ¬¡æ¶ˆè´¹
        ]
        
        for i, features in enumerate(test_customers, 1):
            cluster_result = await self.learning_system.predict(unsupervised_task.id, features)
            print(f"å®¢æˆ· {i}: {features} -> èšç±»: {cluster_result.predictions[0]}")
            print(f"  è½®å»“ç³»æ•°: {cluster_result.accuracy:.3f}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.learning_system.get_stats()
        print(f"\nå­¦ä¹ ç³»ç»Ÿç»Ÿè®¡:")
        print(f"  æ€»ä»»åŠ¡æ•°: {stats['total_tasks']}")
        print(f"  å®Œæˆä»»åŠ¡æ•°: {stats['completed_tasks']}")
        print(f"  æ€»æ•°æ®ç‚¹æ•°: {stats['total_data_points']}")
        print(f"  æ€»ç»“æœæ•°: {stats['total_results']}")
    
    async def demo_retrieval_system(self):
        """æ¼”ç¤ºæ£€ç´¢ç³»ç»Ÿ"""
        print("\n" + "="*60)
        print("ğŸ” æ£€ç´¢ç³»ç»Ÿæ¼”ç¤º")
        print("="*60)
        
        # æ·»åŠ æ–‡æ¡£
        print("æ·»åŠ æ–‡æ¡£...")
        documents = [
            Document(
                title="äººå·¥æ™ºèƒ½æ¦‚è¿°",
                content="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚",
                source="AI_101"
            ),
            Document(
                title="æœºå™¨å­¦ä¹ åŸºç¡€",
                content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
                source="ML_Guide"
            ),
            Document(
                title="æ·±åº¦å­¦ä¹ åŸç†",
                content="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚",
                source="DL_Book"
            ),
            Document(
                title="è‡ªç„¶è¯­è¨€å¤„ç†",
                content="è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
                source="NLP_Tutorial"
            ),
            Document(
                title="è®¡ç®—æœºè§†è§‰",
                content="è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»å›¾åƒå’Œè§†é¢‘ä¸­æå–æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚",
                source="CV_Handbook"
            ),
            Document(
                title="Pythonç¼–ç¨‹æŒ‡å—",
                content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ï¼Œå¹¿æ³›åº”ç”¨äºæ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½é¢†åŸŸã€‚",
                source="Python_Guide"
            ),
            Document(
                title="TensorFlowæ¡†æ¶",
                content="TensorFlowæ˜¯Googleå¼€å‘çš„å¼€æºæœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒæ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œæ¨¡å‹çš„æ„å»ºå’Œè®­ç»ƒã€‚",
                source="TF_Docs"
            ),
            Document(
                title="PyTorchæ•™ç¨‹",
                content="PyTorchæ˜¯Facebookå¼€å‘çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä»¥å…¶åŠ¨æ€è®¡ç®—å›¾å’Œæ˜“ç”¨æ€§è€Œé—»åã€‚",
                source="PT_Tutorial"
            )
        ]
        
        for doc in documents:
            await self.retrieval_system.add_document(doc)
            print(f"âœ“ æ·»åŠ æ–‡æ¡£: {doc.title}")
        
        # å…³é”®è¯æœç´¢
        print("\nå…³é”®è¯æœç´¢:")
        keyword_queries = [
            "äººå·¥æ™ºèƒ½ æœºå™¨å­¦ä¹ ",
            "æ·±åº¦å­¦ä¹  ç¥ç»ç½‘ç»œ",
            "Python ç¼–ç¨‹"
        ]
        
        for query_text in keyword_queries:
            query = Query(
                text=query_text,
                query_type=RetrievalType.KEYWORD,
                limit=3
            )
            
            results = await self.retrieval_system.search(query)
            print(f"\næœç´¢ '{query_text}':")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result.document.title} (åˆ†æ•°: {result.relevance_score:.3f})")
                print(f"     è§£é‡Š: {result.explanation}")
        
        # å‘é‡æœç´¢
        print("\nå‘é‡æœç´¢:")
        vector_queries = [
            "å¦‚ä½•è®©è®¡ç®—æœºå­¦ä¹ ",
            "å›¾åƒè¯†åˆ«æŠ€æœ¯",
            "è‡ªç„¶è¯­è¨€ç†è§£"
        ]
        
        for query_text in vector_queries:
            query = Query(
                text=query_text,
                query_type=RetrievalType.VECTOR,
                limit=3
            )
            
            results = await self.retrieval_system.search(query)
            print(f"\næœç´¢ '{query_text}':")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result.document.title} (ç›¸ä¼¼åº¦: {result.similarity_score:.3f})")
                print(f"     è§£é‡Š: {result.explanation}")
        
        # æ··åˆæœç´¢
        print("\næ··åˆæœç´¢:")
        hybrid_queries = [
            "ç¥ç»ç½‘ç»œ æ·±åº¦å­¦ä¹ ",
            "Python æœºå™¨å­¦ä¹  æ¡†æ¶",
            "äººå·¥æ™ºèƒ½ åº”ç”¨ æŠ€æœ¯"
        ]
        
        for query_text in hybrid_queries:
            query = Query(
                text=query_text,
                query_type=RetrievalType.HYBRID,
                limit=3
            )
            
            results = await self.retrieval_system.search(query)
            print(f"\næœç´¢ '{query_text}':")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result.document.title} (ç›¸å…³æ€§: {result.relevance_score:.3f})")
                print(f"     è§£é‡Š: {result.explanation}")
        
        # è¯­ä¹‰æœç´¢
        print("\nè¯­ä¹‰æœç´¢:")
        semantic_queries = [
            "è®©æœºå™¨åƒäººä¸€æ ·æ€è€ƒ",
            "ä»å›¾åƒä¸­æå–ä¿¡æ¯",
            "ç†è§£å’Œç”Ÿæˆè¯­è¨€"
        ]
        
        for query_text in semantic_queries:
            query = Query(
                text=query_text,
                query_type=RetrievalType.SEMANTIC,
                limit=3
            )
            
            results = await self.retrieval_system.search(query)
            print(f"\næœç´¢ '{query_text}':")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result.document.title} (ç›¸ä¼¼åº¦: {result.similarity_score:.3f})")
                print(f"     è§£é‡Š: {result.explanation}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.retrieval_system.get_stats()
        print(f"\næ£€ç´¢ç³»ç»Ÿç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
        print(f"  æ€»æŸ¥è¯¢æ•°: {stats['total_queries']}")
        print(f"  æ€»ç»“æœæ•°: {stats['total_results']}")
        print(f"  å€’æ’ç´¢å¼•å¤§å°: {stats['inverted_index_size']}")
        print(f"  å‘é‡ç´¢å¼•å¤§å°: {stats['vector_index_size']}")
        print(f"  å¹³å‡æ–‡æ¡£é•¿åº¦: {stats['average_document_length']:.1f}")
        print(f"  å¹³å‡å…³é”®è¯æ•°: {stats['average_keywords_per_document']:.1f}")
    
    async def demo_integration(self):
        """æ¼”ç¤ºç³»ç»Ÿé›†æˆ"""
        print("\n" + "="*60)
        print("ğŸ”— ç³»ç»Ÿé›†æˆæ¼”ç¤º")
        print("="*60)
        
        # åœºæ™¯ï¼šæ™ºèƒ½åŠ©æ‰‹å­¦ä¹ ç”¨æˆ·åå¥½å¹¶æ¨èå†…å®¹
        
        print("åœºæ™¯ï¼šæ™ºèƒ½åŠ©æ‰‹å­¦ä¹ ç”¨æˆ·åå¥½å¹¶æ¨èå†…å®¹")
        print("-" * 40)
        
        # 1. è®°å¿†ç³»ç»Ÿï¼šè®°å½•ç”¨æˆ·è¡Œä¸º
        print("\n1. è®°å½•ç”¨æˆ·è¡Œä¸ºåˆ°è®°å¿†ç³»ç»Ÿ...")
        user_behaviors = [
            MemoryEntry(
                content="ç”¨æˆ·å–œæ¬¢é˜…è¯»æœºå™¨å­¦ä¹ ç›¸å…³æ–‡ç« ",
                memory_type=MemoryType.EPISODIC,
                importance=0.8,
                context={"user_id": "user_001", "behavior": "reading", "topic": "ML"}
            ),
            MemoryEntry(
                content="ç”¨æˆ·ç»å¸¸æœç´¢Pythonç¼–ç¨‹é—®é¢˜",
                memory_type=MemoryType.EPISODIC,
                importance=0.7,
                context={"user_id": "user_001", "behavior": "searching", "topic": "Python"}
            ),
            MemoryEntry(
                content="ç”¨æˆ·å¯¹æ·±åº¦å­¦ä¹ æ„Ÿå…´è¶£",
                memory_type=MemoryType.SEMANTIC,
                importance=0.9,
                context={"user_id": "user_001", "interest": "deep_learning"}
            )
        ]
        
        for behavior in user_behaviors:
            await self.memory_system.add_memory(behavior)
            print(f"âœ“ è®°å½•è¡Œä¸º: {behavior.content}")
        
        # 2. çŸ¥è¯†å›¾è°±ï¼šæ„å»ºç”¨æˆ·å…´è¶£å›¾è°±
        print("\n2. æ„å»ºç”¨æˆ·å…´è¶£å›¾è°±...")
        user_entities = [
            Entity(name="ç”¨æˆ·001", entity_type=EntityType.PERSON, attributes={"user_id": "user_001"}),
            Entity(name="æœºå™¨å­¦ä¹ ", entity_type=EntityType.CONCEPT, attributes={"category": "AI"}),
            Entity(name="Python", entity_type=EntityType.CONCEPT, attributes={"type": "ç¼–ç¨‹è¯­è¨€"}),
            Entity(name="æ·±åº¦å­¦ä¹ ", entity_type=EntityType.CONCEPT, attributes={"category": "ML"})
        ]
        
        for entity in user_entities:
            await self.knowledge_graph.add_entity(entity)
        
        # åˆ›å»ºç”¨æˆ·å…´è¶£å…³ç³»
        user_relations = [
            Relation(
                source_entity_id=user_entities[0].id,  # ç”¨æˆ·001
                target_entity_id=user_entities[1].id,  # æœºå™¨å­¦ä¹ 
                relation_type=RelationType.RELATED_TO,
                confidence=0.8
            ),
            Relation(
                source_entity_id=user_entities[0].id,  # ç”¨æˆ·001
                target_entity_id=user_entities[2].id,  # Python
                relation_type=RelationType.RELATED_TO,
                confidence=0.7
            ),
            Relation(
                source_entity_id=user_entities[1].id,  # æœºå™¨å­¦ä¹ 
                target_entity_id=user_entities[3].id,  # æ·±åº¦å­¦ä¹ 
                relation_type=RelationType.PART_OF,
                confidence=0.9
            )
        ]
        
        for relation in user_relations:
            await self.knowledge_graph.add_relation(relation)
            print(f"âœ“ åˆ›å»ºå…³ç³»: {relation.relation_type.value}")
        
        # 3. æ¨ç†å¼•æ“ï¼šæ¨ç†ç”¨æˆ·åå¥½
        print("\n3. æ¨ç†ç”¨æˆ·åå¥½...")
        preference_rules = [
            Rule(
                name="å…´è¶£ä¼ æ’­è§„åˆ™",
                rule_type=RuleType.IF_THEN,
                antecedent=["å¯¹æœºå™¨å­¦ä¹ æ„Ÿå…´è¶£"],
                consequent=["å¯¹æ·±åº¦å­¦ä¹ æ„Ÿå…´è¶£"],
                confidence=0.8
            ),
            Rule(
                name="ç¼–ç¨‹è¯­è¨€åå¥½è§„åˆ™",
                rule_type=RuleType.IF_THEN,
                antecedent=["ä½¿ç”¨Pythonç¼–ç¨‹"],
                consequent=["å¯¹Pythonæ¡†æ¶æ„Ÿå…´è¶£"],
                confidence=0.7
            )
        ]
        
        for rule in preference_rules:
            await self.reasoning_engine.add_rule(rule)
        
        # æ·»åŠ ç”¨æˆ·äº‹å®
        user_facts = [
            Fact(statement="å¯¹æœºå™¨å­¦ä¹ æ„Ÿå…´è¶£", confidence=0.8),
            Fact(statement="ä½¿ç”¨Pythonç¼–ç¨‹", confidence=0.7)
        ]
        
        for fact in user_facts:
            await self.reasoning_engine.add_fact(fact)
        
        # æ¨ç†ç”¨æˆ·åå¥½
        preference_results = await self.reasoning_engine.reason(
            ReasoningType.DEDUCTIVE,
            InferenceMethod.FORWARD_CHAINING
        )
        
        print("æ¨ç†ç»“æœ:")
        for result in preference_results:
            print(f"  - {result.conclusion} (ç½®ä¿¡åº¦: {result.confidence:.2f})")
        
        # 4. å­¦ä¹ ç³»ç»Ÿï¼šå­¦ä¹ ç”¨æˆ·è¡Œä¸ºæ¨¡å¼
        print("\n4. å­¦ä¹ ç”¨æˆ·è¡Œä¸ºæ¨¡å¼...")
        behavior_task = LearningTask(
            name="ç”¨æˆ·è¡Œä¸ºé¢„æµ‹",
            learning_type=LearningType.SUPERVISED,
            algorithm=LearningAlgorithm.LINEAR_REGRESSION,
            mode=LearningMode.BATCH
        )
        
        await self.learning_system.create_task(behavior_task)
        
        # æ·»åŠ ç”¨æˆ·è¡Œä¸ºæ•°æ®
        behavior_data = [
            TrainingData(features=[1, 0, 1], label=0.8),  # [ML, Python, DL] -> å…´è¶£åº¦
            TrainingData(features=[1, 1, 0], label=0.7),
            TrainingData(features=[0, 1, 1], label=0.6),
            TrainingData(features=[1, 1, 1], label=0.9)
        ]
        
        for data in behavior_data:
            await self.learning_system.add_training_data(behavior_task.id, data)
        
        await self.learning_system.train_model(behavior_task.id)
        print("âœ“ ç”¨æˆ·è¡Œä¸ºæ¨¡å¼å­¦ä¹ å®Œæˆ")
        
        # 5. æ£€ç´¢ç³»ç»Ÿï¼šæ¨èç›¸å…³å†…å®¹
        print("\n5. æ¨èç›¸å…³å†…å®¹...")
        recommendation_query = Query(
            text="æœºå™¨å­¦ä¹  Python æ·±åº¦å­¦ä¹ ",
            query_type=RetrievalType.HYBRID,
            limit=3
        )
        
        recommendations = await self.retrieval_system.search(recommendation_query)
        print("æ¨èå†…å®¹:")
        for i, result in enumerate(recommendations, 1):
            print(f"  {i}. {result.document.title}")
            print(f"     ç›¸å…³æ€§: {result.relevance_score:.3f}")
            print(f"     å†…å®¹: {result.document.content[:50]}...")
        
        # 6. ç»¼åˆæ¨è
        print("\n6. ç»¼åˆæ¨èç»“æœ...")
        print("åŸºäºç”¨æˆ·è¡Œä¸ºã€å…´è¶£å›¾è°±ã€æ¨ç†ç»“æœå’Œå­¦ä¹ æ¨¡å‹çš„ç»¼åˆæ¨è:")
        
        # ç»“åˆè®°å¿†ç³»ç»Ÿæ£€ç´¢
        memory_query = MemoryQuery(
            query_text="æœºå™¨å­¦ä¹ ",
            memory_types=[MemoryType.EPISODIC, MemoryType.SEMANTIC],
            limit=3
        )
        memory_results = await self.memory_system.retrieve_memories(memory_query)
        
        print(f"\nè®°å¿†ç³»ç»Ÿæ¨è ({len(memory_results)} æ¡):")
        for result in memory_results:
            print(f"  - {result.content}")
        
        # ç»“åˆçŸ¥è¯†å›¾è°±æŸ¥è¯¢
        kg_query = GraphQuery(
            query_type="entity_search",
            parameters={"name": "æœºå™¨å­¦ä¹ ", "entity_type": "æ¦‚å¿µ"},
            limit=3
        )
        kg_results = await self.knowledge_graph.query(kg_query)
        
        print(f"\nçŸ¥è¯†å›¾è°±æ¨è ({len(kg_results.get('entities', []))} ä¸ªå®ä½“):")
        for entity in kg_results.get('entities', []):
            print(f"  - {entity.name} ({entity.entity_type.value})")
        
        print("\nâœ“ ç³»ç»Ÿé›†æˆæ¼”ç¤ºå®Œæˆ")
    
    async def run_full_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸš€ ç¬¬4ç«  è®°å¿†ä¸æ¨ç†ç³»ç»Ÿæ„å»º - å®Œæ•´æ¼”ç¤º")
        print("=" * 80)
        
        try:
            await self.start()
            
            # è¿è¡Œå„ä¸ªç³»ç»Ÿæ¼”ç¤º
            await self.demo_memory_system()
            await self.demo_knowledge_graph()
            await self.demo_reasoning_engine()
            await self.demo_learning_system()
            await self.demo_retrieval_system()
            await self.demo_integration()
            
            print("\n" + "=" * 80)
            print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
            print("=" * 80)
            
        except Exception as e:
            logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        
        finally:
            await self.stop()

# ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°"""
    demo = MemoryReasoningDemo()
    await demo.run_full_demo()

if __name__ == "__main__":
    asyncio.run(main())

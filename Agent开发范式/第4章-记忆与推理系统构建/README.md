# ç¬¬4ç«  è®°å¿†ä¸æ¨ç†ç³»ç»Ÿæ„å»ºï¼šæ™ºèƒ½ä½“çš„è®¤çŸ¥æ ¸å¿ƒ

> æ·±å…¥æ¢è®¨æ™ºèƒ½ä½“çš„è®°å¿†ç®¡ç†ã€çŸ¥è¯†å­˜å‚¨å’Œæ¨ç†å†³ç­–æœºåˆ¶

## ğŸ“‹ ç« èŠ‚æ¦‚è§ˆ

æœ¬ç« å°†æ·±å…¥åˆ†ææ™ºèƒ½ä½“çš„è®°å¿†ä¸æ¨ç†ç³»ç»Ÿï¼Œè¿™æ˜¯æ™ºèƒ½ä½“è®¤çŸ¥èƒ½åŠ›çš„æ ¸å¿ƒã€‚æˆ‘ä»¬å°†ä»è®°å¿†ç³»ç»Ÿæ¶æ„å…¥æ‰‹ï¼Œé€æ­¥è®²è§£çŸ¥è¯†åº“ç®¡ç†ã€æ¨ç†å¼•æ“ã€å­¦ä¹ ç³»ç»Ÿã€æ£€ç´¢ç³»ç»Ÿç­‰æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œè¯»è€…å°†èƒ½å¤Ÿè®¾è®¡å’Œå®ç°é«˜æ•ˆã€æ™ºèƒ½çš„è®°å¿†ä¸æ¨ç†ç³»ç»Ÿã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

- ç†è§£æ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿçš„æ¶æ„è®¾è®¡åŸç†
- æŒæ¡çŸ¥è¯†åº“ç®¡ç†å’Œå­˜å‚¨ç­–ç•¥
- å­¦ä¼šè®¾è®¡å’Œå®ç°æ¨ç†å¼•æ“
- å»ºç«‹å­¦ä¹ ç³»ç»Ÿå’ŒçŸ¥è¯†æ›´æ–°æœºåˆ¶
- æŒæ¡æ£€ç´¢ç³»ç»Ÿå’ŒçŸ¥è¯†å‘ç°æŠ€æœ¯

## ğŸ“– ç« èŠ‚ç»“æ„

#### 1. [è®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡](#1-è®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡)
æ·±å…¥æ¢è®¨æ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿçš„æ•´ä½“æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬çŸ­æœŸè®°å¿†ã€é•¿æœŸè®°å¿†ã€å·¥ä½œè®°å¿†å’Œå…ƒè®°å¿†çš„åˆ†å±‚ç»“æ„ã€‚æˆ‘ä»¬å°†è¯¦ç»†åˆ†æå„ç§è®°å¿†ç±»å‹çš„ç‰¹ç‚¹ã€å­˜å‚¨æœºåˆ¶å’Œè®¿é—®æ¨¡å¼ï¼Œå­¦ä¹ å¦‚ä½•è®¾è®¡é«˜æ•ˆã€å¯æ‰©å±•çš„è®°å¿†ç³»ç»Ÿã€‚é€šè¿‡æ¶æ„å›¾å’ŒæŠ€æœ¯åˆ†æï¼Œå¸®åŠ©è¯»è€…å»ºç«‹å¯¹æ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿçš„æ•´ä½“è®¤çŸ¥æ¡†æ¶ã€‚

#### 2. [çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ](#2-çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ)
è¯¦ç»†ä»‹ç»æ™ºèƒ½ä½“çŸ¥è¯†åº“çš„æ„å»ºã€ç®¡ç†å’Œç»´æŠ¤æ–¹æ³•ã€‚æˆ‘ä»¬å°†å­¦ä¹ çŸ¥è¯†è¡¨ç¤ºã€çŸ¥è¯†å­˜å‚¨ã€çŸ¥è¯†æ›´æ–°ã€çŸ¥è¯†éªŒè¯ç­‰å…³é”®æŠ€æœ¯ï¼ŒæŒæ¡å…³ç³»æ•°æ®åº“ã€å‘é‡æ•°æ®åº“ã€å›¾æ•°æ®åº“ç­‰ä¸åŒå­˜å‚¨æŠ€æœ¯çš„åº”ç”¨ã€‚é€šè¿‡å®é™…æ¡ˆä¾‹å±•ç¤ºå¦‚ä½•æ„å»ºç»“æ„åŒ–çš„çŸ¥è¯†åº“ç³»ç»Ÿã€‚

#### 3. [æ¨ç†å¼•æ“å®ç°](#3-æ¨ç†å¼•æ“å®ç°)
æ·±å…¥åˆ†ææ™ºèƒ½ä½“æ¨ç†å¼•æ“çš„è®¾è®¡åŸç†å’Œå®ç°æ–¹æ³•ã€‚æˆ‘ä»¬å°†å­¦ä¹ æ¼”ç»æ¨ç†ã€å½’çº³æ¨ç†ã€ç±»æ¯”æ¨ç†ã€å¸¸è¯†æ¨ç†ç­‰ä¸åŒçš„æ¨ç†ç±»å‹ï¼ŒæŒæ¡å‰å‘æ¨ç†ã€åå‘æ¨ç†ã€å½’ç»“æ¨ç†ç­‰æ¨ç†ç®—æ³•çš„å®ç°ã€‚é€šè¿‡å®é™…æ¡ˆä¾‹å±•ç¤ºå¦‚ä½•æ„å»ºå¼ºå¤§çš„æ¨ç†å¼•æ“ã€‚

#### 4. [å­¦ä¹ ç³»ç»Ÿæ„å»º](#4-å­¦ä¹ ç³»ç»Ÿæ„å»º)
å…¨é¢è§£ææ™ºèƒ½ä½“å­¦ä¹ ç³»ç»Ÿçš„è®¾è®¡æ–¹æ³•å’Œå®ç°æŠ€æœ¯ã€‚æˆ‘ä»¬å°†å­¦ä¹ ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€è¿ç§»å­¦ä¹ ç­‰ä¸åŒçš„å­¦ä¹ æ¨¡å¼ï¼ŒæŒæ¡ç¥ç»ç½‘ç»œã€å†³ç­–æ ‘ã€è´å¶æ–¯ç­‰å­¦ä¹ ç®—æ³•çš„åº”ç”¨ã€‚é€šè¿‡å®é™…æ¡ˆä¾‹å±•ç¤ºå¦‚ä½•æ„å»ºè‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿã€‚

#### 5. [æ£€ç´¢ç³»ç»Ÿè®¾è®¡](#5-æ£€ç´¢ç³»ç»Ÿè®¾è®¡)
è¯¦ç»†ä»‹ç»æ™ºèƒ½ä½“æ£€ç´¢ç³»ç»Ÿçš„è®¾è®¡åŸç†å’Œä¼˜åŒ–æ–¹æ³•ã€‚æˆ‘ä»¬å°†å­¦ä¹ å…³é”®è¯æ£€ç´¢ã€è¯­ä¹‰æ£€ç´¢ã€å‘é‡æ£€ç´¢ã€æ··åˆæ£€ç´¢ç­‰ä¸åŒçš„æ£€ç´¢æŠ€æœ¯ï¼ŒæŒæ¡å€’æ’ç´¢å¼•ã€å‘é‡ç´¢å¼•ã€æ ‘å½¢ç´¢å¼•ç­‰ç´¢å¼•ç»“æ„çš„å®ç°ã€‚é€šè¿‡æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–æ¡ˆä¾‹ï¼Œå¸®åŠ©è¯»è€…æ„å»ºé«˜æ•ˆçš„æ£€ç´¢ç³»ç»Ÿã€‚

#### 6. [çŸ¥è¯†å›¾è°±æ„å»º](#6-çŸ¥è¯†å›¾è°±æ„å»º)
æ·±å…¥æ¢è®¨çŸ¥è¯†å›¾è°±çš„æ„å»ºæ–¹æ³•å’ŒæŠ€æœ¯å®ç°ã€‚æˆ‘ä»¬å°†å­¦ä¹ å®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ã€å®ä½“é“¾æ¥ã€çŸ¥è¯†èåˆç­‰æ ¸å¿ƒæŠ€æœ¯ï¼ŒæŒæ¡å›¾æ•°æ®åº“ã€å›¾ç®—æ³•ã€å›¾æŸ¥è¯¢ç­‰æŠ€æœ¯çš„åº”ç”¨ã€‚é€šè¿‡å®é™…æ¡ˆä¾‹å±•ç¤ºå¦‚ä½•æ„å»ºå¤§è§„æ¨¡çš„çŸ¥è¯†å›¾è°±ç³»ç»Ÿã€‚

#### 7. [å®æˆ˜æ¡ˆä¾‹ï¼šæ„å»ºæ™ºèƒ½è®°å¿†ç³»ç»Ÿ](#7-å®æˆ˜æ¡ˆä¾‹æ„å»ºæ™ºèƒ½è®°å¿†ç³»ç»Ÿ)
é€šè¿‡ä¸€ä¸ªå®Œæ•´çš„å®æˆ˜æ¡ˆä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªæ™ºèƒ½è®°å¿†ç³»ç»Ÿã€‚æ¡ˆä¾‹å°†æ¶µç›–éœ€æ±‚åˆ†æã€æ¶æ„è®¾è®¡ã€ç³»ç»Ÿå®ç°ã€æµ‹è¯•éªŒè¯ã€æ€§èƒ½ä¼˜åŒ–ç­‰å®Œæ•´çš„å¼€å‘æµç¨‹ã€‚é€šè¿‡å®é™…çš„é¡¹ç›®å¼€å‘è¿‡ç¨‹ï¼Œå¸®åŠ©è¯»è€…å°†ç†è®ºçŸ¥è¯†è½¬åŒ–ä¸ºå®è·µèƒ½åŠ›ã€‚

#### 8. [æœ€ä½³å®è·µæ€»ç»“](#8-æœ€ä½³å®è·µæ€»ç»“)
æ€»ç»“è®°å¿†ä¸æ¨ç†ç³»ç»Ÿå¼€å‘çš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬è®¾è®¡åŸåˆ™ã€æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨é˜²æŠ¤ã€ç»´æŠ¤ç­–ç•¥ç­‰ã€‚æˆ‘ä»¬å°†åˆ†äº«åœ¨å®é™…é¡¹ç›®ä¸­ç§¯ç´¯çš„ç»éªŒæ•™è®­ï¼Œå¸®åŠ©è¯»è€…é¿å…å¸¸è§çš„é™·é˜±å’Œé—®é¢˜ï¼Œæé«˜ç³»ç»Ÿè´¨é‡å’Œå¼€å‘æ•ˆç‡ã€‚

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```text
ç¬¬4ç« -è®°å¿†ä¸æ¨ç†ç³»ç»Ÿæ„å»º/
â”œâ”€â”€ README.md                    # æœ¬ç« æ¦‚è§ˆå’Œè¯´æ˜
â”œâ”€â”€ code/                        # æ ¸å¿ƒä»£ç å®ç°
â”‚   â”œâ”€â”€ memory_system.py         # è®°å¿†ç³»ç»Ÿæ¶æ„
â”‚   â”œâ”€â”€ knowledge_graph.py      # çŸ¥è¯†å›¾è°±
â”‚   â”œâ”€â”€ reasoning_engine.py     # æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ learning_system.py       # å­¦ä¹ ç³»ç»Ÿ
â”‚   â””â”€â”€ retrieval_system.py      # æ£€ç´¢ç³»ç»Ÿ
â”œâ”€â”€ tests/                       # æµ‹è¯•ç”¨ä¾‹
â”‚   â””â”€â”€ test_memory_reasoning_system.py # è®°å¿†ä¸æ¨ç†ç³»ç»Ÿæµ‹è¯•
â”œâ”€â”€ config/                      # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ memory_reasoning_configs.yaml # è®°å¿†ä¸æ¨ç†ç³»ç»Ÿé…ç½®
â””â”€â”€ examples/                    # æ¼”ç¤ºç¤ºä¾‹
    â””â”€â”€ memory_reasoning_demo.py # è®°å¿†ä¸æ¨ç†æ¼”ç¤º
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…è®°å¿†ä¸æ¨ç†ç›¸å…³ä¾èµ–
pip install asyncio pytest pyyaml
pip install numpy pandas scikit-learn
pip install networkx matplotlib
pip install sqlalchemy redis

# æˆ–ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
python -m venv chapter4_env
source chapter4_env/bin/activate  # Linux/Mac
pip install asyncio pytest pyyaml numpy pandas scikit-learn networkx matplotlib sqlalchemy redis
```

### 2. è¿è¡ŒåŸºç¡€ç¤ºä¾‹

```bash
# è¿è¡Œè®°å¿†ç³»ç»Ÿç¤ºä¾‹
cd code
python memory_system.py

# è¿è¡ŒçŸ¥è¯†å›¾è°±ç¤ºä¾‹
python knowledge_graph.py

# è¿è¡Œæ¨ç†å¼•æ“ç¤ºä¾‹
python reasoning_engine.py

# è¿è¡Œå­¦ä¹ ç³»ç»Ÿç¤ºä¾‹
python learning_system.py

# è¿è¡Œæ£€ç´¢ç³»ç»Ÿç¤ºä¾‹
python retrieval_system.py

# è¿è¡Œå®Œæ•´æ¼”ç¤º
cd examples
python memory_reasoning_demo.py
```

### 3. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
cd tests
python -m pytest test_memory_reasoning_system.py -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
python -m pytest test_memory_reasoning_system.py::test_memory_system_start_stop -v
```

---

## 1. è®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡

### 1.1 è®°å¿†ç³»ç»Ÿæ¦‚è¿°

è®°å¿†ç³»ç»Ÿæ˜¯æ™ºèƒ½ä½“çš„"å¤§è„‘"ï¼Œå®ƒè´Ÿè´£å­˜å‚¨ã€ç®¡ç†å’Œæ£€ç´¢æ™ºèƒ½ä½“çš„çŸ¥è¯†å’Œç»éªŒã€‚ä¸€ä¸ªä¼˜ç§€çš„è®°å¿†ç³»ç»Ÿä¸ä»…éœ€è¦é«˜æ•ˆçš„ä¿¡æ¯å­˜å‚¨å’Œæ£€ç´¢èƒ½åŠ›ï¼Œè¿˜éœ€è¦æ”¯æŒçŸ¥è¯†çš„æ›´æ–°ã€èåˆå’Œæ¨ç†ã€‚è®°å¿†ç³»ç»Ÿçš„è®¾è®¡ç›´æ¥å½±å“åˆ°æ™ºèƒ½ä½“çš„æ™ºèƒ½æ°´å¹³å’Œæ€§èƒ½è¡¨ç°ã€‚

### 1.2 è®°å¿†ç³»ç»Ÿåˆ†å±‚æ¶æ„

#### 1.2.1 çŸ­æœŸè®°å¿†å±‚ (Short-Term Memory)

çŸ­æœŸè®°å¿†å±‚è´Ÿè´£å­˜å‚¨ä¸´æ—¶æ€§çš„ä¿¡æ¯ï¼Œå¦‚å½“å‰å¯¹è¯çš„ä¸Šä¸‹æ–‡ã€ä¸´æ—¶è®¡ç®—ç»“æœç­‰ã€‚çŸ­æœŸè®°å¿†çš„ç‰¹ç‚¹æ˜¯è®¿é—®é€Ÿåº¦å¿«ï¼Œä½†å®¹é‡æœ‰é™ï¼Œæ•°æ®æŒä¹…æ€§çŸ­ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **é«˜é€Ÿè®¿é—®**: æ¯«ç§’çº§çš„è®¿é—®é€Ÿåº¦
- **æœ‰é™å®¹é‡**: é€šå¸¸é™åˆ¶åœ¨å‡ ç™¾åˆ°å‡ åƒä¸ªæ¡ç›®
- **ä¸´æ—¶å­˜å‚¨**: æ•°æ®é€šå¸¸åœ¨ä¼šè¯ç»“æŸåæ¸…é™¤
- **ä¸Šä¸‹æ–‡ç›¸å…³**: ä¸å½“å‰ä»»åŠ¡å’Œå¯¹è¯ç´§å¯†ç›¸å…³

**æŠ€æœ¯å®ç°ï¼š**
```python
class ShortTermMemory:
    """çŸ­æœŸè®°å¿†ç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.max_entries = config.get("max_entries", 1000)
        self.ttl_seconds = config.get("ttl_seconds", 3600)  # 1å°æ—¶TTL
        self.memory_cache = {}
        self.access_stats = {}
        self.cleanup_interval = 300  # 5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡

    async def store(self, memory: Memory) -> str:
        """å­˜å‚¨çŸ­æœŸè®°å¿†"""
        memory_id = f"stm_{uuid.uuid4().hex[:8]}"
        timestamp = datetime.now()

        # 1. æ£€æŸ¥å®¹é‡é™åˆ¶
        if len(self.memory_cache) >= self.max_entries:
            await self._cleanup_expired_memories()

        # 2. å­˜å‚¨è®°å¿†
        memory_entry = {
            "memory_id": memory_id,
            "content": memory.content,
            "metadata": memory.metadata,
            "created_at": timestamp,
            "expires_at": timestamp + timedelta(seconds=self.ttl_seconds),
            "access_count": 0,
            "last_accessed": timestamp
        }

        self.memory_cache[memory_id] = memory_entry

        # 3. æ›´æ–°è®¿é—®ç»Ÿè®¡
        self.access_stats[memory_id] = {
            "store_time": timestamp,
            "access_times": [],
            "total_access_count": 0
        }

        return memory_id

    async def retrieve(self, query: str, limit: int = 10) -> List[Memory]:
        """æ£€ç´¢çŸ­æœŸè®°å¿†"""
        current_time = datetime.now()
        relevant_memories = []

        # 1. æ¸…ç†è¿‡æœŸè®°å¿†
        await self._cleanup_expired_memories()

        # 2. ç›¸ä¼¼åº¦è®¡ç®—å’Œæ’åº
        for memory_id, memory_entry in self.memory_cache.items():
            if current_time < memory_entry["expires_at"]:
                similarity = self._calculate_similarity(query, memory_entry["content"])
                if similarity > 0.5:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    relevant_memories.append((memory_id, similarity, memory_entry))

        # 3. æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›å‰Nä¸ª
        relevant_memories.sort(key=lambda x: x[1], reverse=True)
        results = []
        for memory_id, similarity, memory_entry in relevant_memories[:limit]:
            memory = Memory(
                content=memory_entry["content"],
                metadata=memory_entry["metadata"],
                memory_type=MemoryType.SHORT_TERM
            )
            memory.memory_id = memory_id
            results.append(memory)

            # 4. æ›´æ–°è®¿é—®ç»Ÿè®¡
            self._update_access_stats(memory_id)

        return results

    async def _cleanup_expired_memories(self):
        """æ¸…ç†è¿‡æœŸè®°å¿†"""
        current_time = datetime.now()
        expired_keys = []

        for memory_id, memory_entry in self.memory_cache.items():
            if current_time >= memory_entry["expires_at"]:
                expired_keys.append(memory_id)

        for memory_id in expired_keys:
            del self.memory_cache[memory_id]
            if memory_id in self.access_stats:
                del self.access_stats[memory_id]

        logger.info(f"Cleaned up {len(expired_keys)} expired short-term memories")

    def _calculate_similarity(self, query: str, content: str) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦"""
        # ä½¿ç”¨TF-IDFæˆ–è¯å‘é‡è®¡ç®—ç›¸ä¼¼åº¦
        query_words = set(query.lower().split())
        content_words = set(content.lower().split())

        if not query_words or not content_words:
            return 0.0

        intersection = query_words.intersection(content_words)
        union = query_words.union(content_words)

        return len(intersection) / len(union) if union else 0.0

    def _update_access_stats(self, memory_id: str):
        """æ›´æ–°è®¿é—®ç»Ÿè®¡"""
        if memory_id in self.access_stats:
            stats = self.access_stats[memory_id]
            stats["access_times"].append(datetime.now())
            stats["total_access_count"] += 1
            if memory_id in self.memory_cache:
                self.memory_cache[memory_id]["access_count"] += 1
                self.memory_cache[memory_id]["last_accessed"] = datetime.now()
```

#### 1.2.2 é•¿æœŸè®°å¿†å±‚ (Long-Term Memory)

é•¿æœŸè®°å¿†å±‚è´Ÿè´£å­˜å‚¨é‡è¦çš„çŸ¥è¯†å’Œç»éªŒï¼Œè¿™äº›ä¿¡æ¯éœ€è¦æŒä¹…åŒ–ä¿å­˜ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ”¯æŒå¤æ‚çš„æŸ¥è¯¢å’Œæ¨ç†æ“ä½œã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **æŒä¹…åŒ–å­˜å‚¨**: æ•°æ®æŒä¹…ä¿å­˜ï¼Œä¸ä¼šéšä¼šè¯ç»“æŸè€Œä¸¢å¤±
- **å¤§å®¹é‡**: æ”¯æŒæµ·é‡æ•°æ®å­˜å‚¨
- **ç»“æ„åŒ–**: æ”¯æŒå¤æ‚çš„æ•°æ®ç»“æ„å’Œå…³ç³»
- **å¯æ£€ç´¢**: æ”¯æŒå¤šç§æ£€ç´¢æ–¹å¼å’ŒæŸ¥è¯¢è¯­è¨€

**æŠ€æœ¯å®ç°ï¼š**
```python
class LongTermMemory:
    """é•¿æœŸè®°å¿†ç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.storage_backend = config.get("storage_backend", "postgresql")
        self.vector_db = config.get("vector_db", "chromadb")
        self.index_manager = IndexManager()
        self.knowledge_graph = KnowledgeGraph(config)

        # åˆå§‹åŒ–å­˜å‚¨åç«¯
        if self.storage_backend == "postgresql":
            self.db_engine = create_engine(config["database_url"])
            self.Session = sessionmaker(bind=self.db_engine)
            self._init_database_tables()

    async def store(self, memory: Memory) -> str:
        """å­˜å‚¨é•¿æœŸè®°å¿†"""
        memory_id = f"ltm_{uuid.uuid4().hex[:8]}"
        timestamp = datetime.now()

        # 1. å‘é‡åŒ–å¤„ç†
        if memory.content_type == "text":
            embedding = await self._generate_embedding(memory.content)
        else:
            embedding = None

        # 2. çŸ¥è¯†æå–å’Œå®ä½“è¯†åˆ«
        entities, relations = await self._extract_knowledge(memory.content)

        # 3. å­˜å‚¨åˆ°å…³ç³»æ•°æ®åº“
        memory_record = MemoryRecord(
            memory_id=memory_id,
            content=memory.content,
            content_type=memory.content_type.value,
            embedding=embedding,
            metadata=json.dumps(memory.metadata),
            created_at=timestamp,
            updated_at=timestamp,
            access_count=0,
            importance_score=self._calculate_importance(memory)
        )

        session = self.Session()
        try:
            session.add(memory_record)
            session.commit()

            # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            if embedding is not None:
                await self._store_vector(memory_id, embedding, memory.metadata)

            # 5. æ›´æ–°çŸ¥è¯†å›¾è°±
            for entity in entities:
                await self.knowledge_graph.add_entity(entity)
            for relation in relations:
                await self.knowledge_graph.add_relation(relation)

            # 6. æ›´æ–°ç´¢å¼•
            await self.index_manager.add_to_index(memory_id, memory.content, memory.metadata)

            return memory_id

        except Exception as e:
            session.rollback()
            raise e
        finally:
            session.close()

    async def retrieve(self, query: str, filters: Dict[str, Any] = None, limit: int = 20) -> List[Memory]:
        """æ£€ç´¢é•¿æœŸè®°å¿†"""
        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = await self._generate_embedding(query)

        # 2. å‘é‡æ£€ç´¢
        vector_results = await self._vector_search(query_embedding, limit * 2)

        # 3. å…³é”®è¯æ£€ç´¢
        keyword_results = await self._keyword_search(query, limit * 2)

        # 4. çŸ¥è¯†å›¾è°±æ£€ç´¢
        graph_results = await self._graph_search(query)

        # 5. ç»“æœèåˆå’Œé‡æ’åº
        fused_results = self._fuse_and_rerank(
            vector_results, keyword_results, graph_results, query
        )

        # 6. åº”ç”¨è¿‡æ»¤å™¨å’Œé™åˆ¶
        filtered_results = self._apply_filters(fused_results, filters)
        final_results = filtered_results[:limit]

        # 7. æ„å»ºMemoryå¯¹è±¡
        memories = []
        for result in final_results:
            memory = Memory(
                content=result["content"],
                metadata=json.loads(result["metadata"]),
                memory_type=MemoryType.LONG_TERM
            )
            memory.memory_id = result["memory_id"]
            memories.append(memory)

        # 8. æ›´æ–°è®¿é—®ç»Ÿè®¡
        await self._update_access_stats([r["memory_id"] for r in final_results])

        return memories

    async def _generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        # ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ç”ŸæˆåµŒå…¥
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨BERTã€GPTç­‰æ¨¡å‹
        words = text.lower().split()
        word_counts = {}
        for word in words:
            word_counts[word] = word_counts.get(word, 0) + 1

        # ç®€å•çš„TF-IDFé£æ ¼åµŒå…¥
        total_words = len(words)
        embedding = [word_counts.get(word, 0) / total_words for word in words[:100]]

        # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
        embedding = embedding[:100] + [0.0] * (100 - len(embedding))

        return embedding

    async def _extract_knowledge(self, text: str) -> Tuple[List[Entity], List[Relation]]:
        """ä»æ–‡æœ¬ä¸­æå–çŸ¥è¯†"""
        # ä½¿ç”¨NLPæŠ€æœ¯æå–å®ä½“å’Œå…³ç³»
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨spaCyã€Stanford NLPç­‰å·¥å…·
        entities = []
        relations = []

        # ç®€å•çš„å®ä½“æå–ï¼ˆåŸºäºå¤§å†™å­—æ¯å’Œå¸¸è§æ¨¡å¼ï¼‰
        import re
        entity_patterns = [
            r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b',  # äººåã€åœ°åç­‰
            r'\b\d{4}\b',  # å¹´ä»½
            r'\b\d+\.\d+\b',  # æ•°å­—
        ]

        for pattern in entity_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                entity = Entity(
                    entity_id=f"entity_{uuid.uuid4().hex[:8]}",
                    name=match,
                    entity_type=EntityType.UNKNOWN,
                    confidence=0.8,
                    source_text=text
                )
                entities.append(entity)

        return entities, relations

    def _calculate_importance(self, memory: Memory) -> float:
        """è®¡ç®—è®°å¿†é‡è¦æ€§åˆ†æ•°"""
        importance = 0.0

        # 1. åŸºäºå†…å®¹é•¿åº¦
        content_length = len(memory.content)
        if content_length > 100:
            importance += 0.2
        elif content_length > 50:
            importance += 0.1

        # 2. åŸºäºå…³é”®è¯
        important_keywords = ["é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "å¿…é¡»"]
        for keyword in important_keywords:
            if keyword in memory.content:
                importance += 0.1

        # 3. åŸºäºå…ƒæ•°æ®
        if memory.metadata.get("importance") == "high":
            importance += 0.3
        elif memory.metadata.get("importance") == "medium":
            importance += 0.15

        # 4. åŸºäºæƒ…æ„Ÿåˆ†æ
        sentiment = memory.metadata.get("sentiment")
        if sentiment in ["positive", "negative"]:
            importance += 0.1

        return min(importance, 1.0)
```

#### 1.2.3 å·¥ä½œè®°å¿†å±‚ (Working Memory)

å·¥ä½œè®°å¿†å±‚è´Ÿè´£å­˜å‚¨å½“å‰ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­éœ€è¦ä¸´æ—¶ä½¿ç”¨çš„ä¿¡æ¯ï¼Œå¦‚ä»»åŠ¡ä¸Šä¸‹æ–‡ã€ä¸­é—´è®¡ç®—ç»“æœã€ä¸´æ—¶å˜é‡ç­‰ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **ä»»åŠ¡ç›¸å…³**: ä¸å½“å‰æ‰§è¡Œçš„ä»»åŠ¡ç´§å¯†ç›¸å…³
- **ä¸´æ—¶å­˜å‚¨**: ä»»åŠ¡å®Œæˆåè‡ªåŠ¨æ¸…é™¤
- **å¿«é€Ÿè®¿é—®**: æä¾›æœ€å¿«çš„è®¿é—®é€Ÿåº¦
- **ä¸Šä¸‹æ–‡ç®¡ç†**: ç»´æŠ¤ä»»åŠ¡æ‰§è¡Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯

#### 1.2.4 å…ƒè®°å¿†å±‚ (Meta Memory)

å…ƒè®°å¿†å±‚è´Ÿè´£ç®¡ç†å’Œæ§åˆ¶æ•´ä¸ªè®°å¿†ç³»ç»Ÿï¼ŒåŒ…æ‹¬è®°å¿†çš„ç´¢å¼•ã€ä¼˜åŒ–ã€ç»Ÿè®¡ç­‰åŠŸèƒ½ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- **ç´¢å¼•ç®¡ç†**: ç»´æŠ¤å„ç§ç´¢å¼•ç»“æ„
- **æ€§èƒ½ä¼˜åŒ–**: è‡ªåŠ¨ä¼˜åŒ–å­˜å‚¨å’Œæ£€ç´¢æ€§èƒ½
- **ç»Ÿè®¡åˆ†æ**: æä¾›è®°å¿†ä½¿ç”¨çš„ç»Ÿè®¡ä¿¡æ¯
- **ç­–ç•¥ç®¡ç†**: ç®¡ç†è®°å¿†çš„å­˜å‚¨å’Œæ£€ç´¢ç­–ç•¥

### 1.3 è®°å¿†ç³»ç»Ÿé›†æˆ

```python
class IntegratedMemorySystem:
    """é›†æˆè®°å¿†ç³»ç»Ÿ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.short_term_memory = ShortTermMemory(config.get("short_term", {}))
        self.long_term_memory = LongTermMemory(config.get("long_term", {}))
        self.working_memory = WorkingMemory(config.get("working", {}))
        self.meta_memory = MetaMemory(config.get("meta", {}))
        self.memory_router = MemoryRouter()

    async def store_memory(self, memory: Memory, storage_hint: str = None) -> str:
        """å­˜å‚¨è®°å¿†åˆ°åˆé€‚çš„å­˜å‚¨å±‚"""
        # 1. ç¡®å®šå­˜å‚¨ç­–ç•¥
        storage_strategy = self.memory_router.determine_storage_strategy(
            memory, storage_hint
        )

        # 2. æ‰§è¡Œå­˜å‚¨æ“ä½œ
        if storage_strategy == StorageStrategy.SHORT_TERM:
            memory_id = await self.short_term_memory.store(memory)
        elif storage_strategy == StorageStrategy.LONG_TERM:
            memory_id = await self.long_term_memory.store(memory)
        elif storage_strategy == StorageStrategy.WORKING:
            memory_id = await self.working_memory.store(memory)
        elif storage_strategy == StorageStrategy.HYBRID:
            # åŒæ—¶å­˜å‚¨åˆ°å¤šä¸ªå­˜å‚¨å±‚
            memory_id = await self.short_term_memory.store(memory)
            await self.long_term_memory.store(memory)
        else:
            raise ValueError(f"Unknown storage strategy: {storage_strategy}")

        # 3. æ›´æ–°å…ƒè®°å¿†
        await self.meta_memory.record_storage_operation(
            memory_id, storage_strategy, memory
        )

        return memory_id

    async def retrieve_memory(self, query: str, context: Dict[str, Any] = None) -> List[Memory]:
        """ä»å¤šä¸ªå­˜å‚¨å±‚æ£€ç´¢è®°å¿†"""
        # 1. åˆ†ææŸ¥è¯¢ä¸Šä¸‹æ–‡
        query_context = self.memory_router.analyze_query_context(query, context)

        # 2. å¹¶è¡Œæ£€ç´¢
        retrieval_tasks = []

        # ä»å·¥ä½œè®°å¿†æ£€ç´¢
        if query_context.need_working_memory:
            retrieval_tasks.append(
                self.working_memory.retrieve(query, query_context.working_memory_filters)
            )

        # ä»çŸ­æœŸè®°å¿†æ£€ç´¢
        if query_context.need_short_term_memory:
            retrieval_tasks.append(
                self.short_term_memory.retrieve(query, query_context.short_term_memory_limit)
            )

        # ä»é•¿æœŸè®°å¿†æ£€ç´¢
        if query_context.need_long_term_memory:
            retrieval_tasks.append(
                self.long_term_memory.retrieve(query, query_context.long_term_memory_filters)
            )

        # 3. æ‰§è¡Œå¹¶è¡Œæ£€ç´¢
        retrieval_results = await asyncio.gather(*retrieval_tasks, return_exceptions=True)

        # 4. ç»“æœèåˆå’Œé‡æ’åº
        all_memories = []
        for result in retrieval_results:
            if isinstance(result, Exception):
                logger.error(f"Retrieval error: {result}")
            else:
                all_memories.extend(result)

        # 5. é‡æ’åºå’Œå»é‡
        ranked_memories = self._rank_and_deduplicate(all_memories, query, context)

        return ranked_memories

    def _rank_and_deduplicate(self, memories: List[Memory], query: str, context: Dict[str, Any]) -> List[Memory]:
        """å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åºå’Œå»é‡"""
        # 1. è®¡ç®—ç›¸å…³åº¦åˆ†æ•°
        memory_scores = []
        for memory in memories:
            score = self._calculate_relevance_score(memory, query, context)
            memory_scores.append((memory, score))

        # 2. æŒ‰åˆ†æ•°æ’åº
        memory_scores.sort(key=lambda x: x[1], reverse=True)

        # 3. å»é‡å¤„ç†
        seen_content = set()
        unique_memories = []
        for memory, score in memory_scores:
            content_hash = hashlib.md5(memory.content.encode()).hexdigest()
            if content_hash not in seen_content:
                seen_content.add(content_hash)
                unique_memories.append(memory)

        return unique_memories

    def _calculate_relevance_score(self, memory: Memory, query: str, context: Dict[str, Any]) -> float:
        """è®¡ç®—è®°å¿†ä¸æŸ¥è¯¢çš„ç›¸å…³åº¦åˆ†æ•°"""
        score = 0.0

        # 1. å†…å®¹ç›¸ä¼¼åº¦
        content_similarity = self._calculate_content_similarity(memory.content, query)
        score += content_similarity * 0.6

        # 2. æ—¶é—´è¡°å‡
        time_decay = self._calculate_time_decay(memory)
        score += time_decay * 0.2

        # 3. è®¿é—®é¢‘ç‡
        access_frequency = self._calculate_access_frequency(memory)
        score += access_frequency * 0.1

        # 4. ä¸Šä¸‹æ–‡åŒ¹é…
        context_match = self._calculate_context_match(memory, context)
        score += context_match * 0.1

        return score

    async def consolidate_memories(self):
        """è®°å¿†æ•´åˆï¼šå°†çŸ­æœŸè®°å¿†ä¸­çš„é‡è¦å†…å®¹è½¬ç§»åˆ°é•¿æœŸè®°å¿†"""
        # 1. è¯†åˆ«éœ€è¦æ•´åˆçš„è®°å¿†
        candidates = await self.short_term_memory.get_consolidation_candidates()

        # 2. è¯„ä¼°é‡è¦æ€§
        important_memories = []
        for memory in candidates:
            importance_score = await self._evaluate_memory_importance(memory)
            if importance_score > 0.7:  # é‡è¦æ€§é˜ˆå€¼
                important_memories.append(memory)

        # 3. è½¬ç§»åˆ°é•¿æœŸè®°å¿†
        for memory in important_memories:
            await self.long_term_memory.store(memory)
            await self.short_term_memory.remove_memory(memory.memory_id)

        logger.info(f"Consolidated {len(important_memories)} memories from short-term to long-term")

    async def _evaluate_memory_importance(self, memory: Memory) -> float:
        """è¯„ä¼°è®°å¿†é‡è¦æ€§"""
        importance = 0.0

        # 1. è®¿é—®é¢‘ç‡
        access_stats = await self.meta_memory.get_access_stats(memory.memory_id)
        if access_stats and access_stats.access_count > 5:
            importance += 0.3

        # 2. å†…å®¹ç‰¹å¾
        if len(memory.content) > 100:
            importance += 0.2

        # 3. æƒ…æ„Ÿåˆ†æ
        sentiment = memory.metadata.get("sentiment")
        if sentiment in ["positive", "negative"]:
            importance += 0.2

        # 4. æ—¶é—´æ–°é²œåº¦
        age = datetime.now() - memory.created_at
        if age.days < 7:  # ä¸€å‘¨å†…çš„è®°å¿†
            importance += 0.2

        # 5. ç”¨æˆ·æ ‡è®°
        if memory.metadata.get("user_marked_important"):
            importance += 0.3

        return min(importance, 1.0)
```

## 2. çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ

### 2.1 çŸ¥è¯†è¡¨ç¤ºæ–¹æ³•

#### 2.1.1 ç¬¦å·è¡¨ç¤º (Symbolic Representation)

ç¬¦å·è¡¨ç¤ºæ˜¯æœ€ä¼ ç»Ÿå’Œç›´è§‚çš„çŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒä½¿ç”¨ç¬¦å·ã€é€»è¾‘è§„åˆ™å’Œè°“è¯æ¥è¡¨ç¤ºçŸ¥è¯†ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- **å¯è§£é‡Šæ€§**: çŸ¥è¯†è¡¨ç¤ºå½¢å¼æ¸…æ™°ï¼Œæ˜“äºç†è§£
- **é€»è¾‘æ¨ç†**: æ”¯æŒä¸¥æ ¼çš„é€»è¾‘æ¨ç†
- **ç²¾ç¡®æ€§**: èƒ½å¤Ÿç²¾ç¡®è¡¨ç¤ºæ¦‚å¿µå’Œå…³ç³»
- **å¯æ‰©å±•æ€§**: å¯ä»¥æ–¹ä¾¿åœ°æ·»åŠ æ–°çš„è§„åˆ™å’Œäº‹å®

**æŠ€æœ¯å®ç°ï¼š**
```python
class SymbolicKnowledgeBase:
    """ç¬¦å·çŸ¥è¯†åº“"""

    def __init__(self):
        self.facts: Set[Fact] = set()
        self.rules: List[Rule] = []
        self.predicates: Dict[str, Predicate] = {}
        self.inference_engine = InferenceEngine()

    def add_fact(self, fact: Fact):
        """æ·»åŠ äº‹å®"""
        self.facts.add(fact)
        logger.info(f"Added fact: {fact}")

    def add_rule(self, rule: Rule):
        """æ·»åŠ è§„åˆ™"""
        self.rules.append(rule)
        logger.info(f"Added rule: {rule}")

    def query(self, query: Query) -> List[Binding]:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        return self.inference_engine.query(self.facts, self.rules, query)

    def reason_forward(self) -> Set[Fact]:
        """å‰å‘æ¨ç†"""
        new_facts = set()
        changed = True

        while changed:
            changed = False
            for rule in self.rules:
                # æ£€æŸ¥è§„åˆ™æ¡ä»¶æ˜¯å¦æ»¡è¶³
                bindings = self._match_rule_conditions(rule)
                if bindings:
                    # åº”ç”¨è§„åˆ™ç”Ÿæˆæ–°äº‹å®
                    for binding in bindings:
                        new_fact = self._apply_rule(rule, binding)
                        if new_fact not in self.facts and new_fact not in new_facts:
                            new_facts.add(new_fact)
                            changed = True

        self.facts.update(new_facts)
        return new_facts

class Fact:
    """äº‹å®ç±»"""

    def __init__(self, predicate: str, arguments: List[str], confidence: float = 1.0):
        self.predicate = predicate
        self.arguments = arguments
        self.confidence = confidence
        self.timestamp = datetime.now()

    def __str__(self):
        return f"{self.predicate}({', '.join(self.arguments)})"

    def __hash__(self):
        return hash((self.predicate, tuple(self.arguments)))

    def __eq__(self, other):
        if not isinstance(other, Fact):
            return False
        return (self.predicate, self.arguments) == (other.predicate, other.arguments)

class Rule:
    """è§„åˆ™ç±»"""

    def __init__(self, name: str, conditions: List[Fact], conclusions: List[Fact], confidence: float = 1.0):
        self.name = name
        self.conditions = conditions
        self.conclusions = conclusions
        self.confidence = confidence

    def __str__(self):
        conditions_str = " âˆ§ ".join(str(cond) for cond in self.conditions)
        conclusions_str = " âˆ§ ".join(str(conc) for conc in self.conclusions)
        return f"{conditions_str} â†’ {conclusions_str}"
```

#### 2.1.2 å‘é‡è¡¨ç¤º (Vector Representation)

å‘é‡è¡¨ç¤ºä½¿ç”¨æ•°å€¼å‘é‡æ¥è¡¨ç¤ºçŸ¥è¯†å’Œæ¦‚å¿µï¼Œé€šè¿‡å‘é‡ç©ºé—´çš„å‡ ä½•å…³ç³»æ¥è¡¨ç¤ºè¯­ä¹‰å…³ç³»ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- **è¯­ä¹‰è¡¨ç¤º**: èƒ½å¤Ÿæ•è·è¯è¯­å’Œæ¦‚å¿µçš„è¯­ä¹‰ä¿¡æ¯
- **ç›¸ä¼¼åº¦è®¡ç®—**: æ–¹ä¾¿è®¡ç®—æ¦‚å¿µé—´çš„ç›¸ä¼¼åº¦
- **æœºå™¨å­¦ä¹ **: ä¸æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ ç®—æ³•å…¼å®¹
- **ç»´åº¦å¤„ç†**: èƒ½å¤Ÿå¤„ç†é«˜ç»´ç‰¹å¾ç©ºé—´

**æŠ€æœ¯å®ç°ï¼š**
```python
class VectorKnowledgeBase:
    """å‘é‡çŸ¥è¯†åº“"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.embedding_dim = config.get("embedding_dim", 768)
        self.vector_db = VectorDatabase(config.get("vector_db", {}))
        self.embedding_model = EmbeddingModel(config.get("embedding_model", {}))
        self.semantic_index = SemanticIndex()

    async def add_knowledge(self, text: str, metadata: Dict[str, Any] = None) -> str:
        """æ·»åŠ çŸ¥è¯†åˆ°å‘é‡çŸ¥è¯†åº“"""
        # 1. ç”ŸæˆåµŒå…¥å‘é‡
        embedding = await self.embedding_model.embed(text)

        # 2. æå–å…³é”®ä¿¡æ¯
        entities = await self._extract_entities(text)
        keywords = await self._extract_keywords(text)

        # 3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        knowledge_id = f"vec_{uuid.uuid4().hex[:8]}"
        knowledge_entry = VectorKnowledgeEntry(
            knowledge_id=knowledge_id,
            text=text,
            embedding=embedding,
            entities=entities,
            keywords=keywords,
            metadata=metadata or {},
            created_at=datetime.now()
        )

        await self.vector_db.add_entry(knowledge_entry)

        # 4. æ›´æ–°è¯­ä¹‰ç´¢å¼•
        await self.semantic_index.add_entry(knowledge_id, text, entities, keywords)

        return knowledge_id

    async def semantic_search(self, query: str, limit: int = 10) -> List[KnowledgeSearchResult]:
        """è¯­ä¹‰æœç´¢"""
        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = await self.embedding_model.embed(query)

        # 2. å‘é‡ç›¸ä¼¼åº¦æœç´¢
        vector_results = await self.vector_db.similarity_search(query_embedding, limit * 2)

        # 3. è¯­ä¹‰æ‰©å±•æœç´¢
        expanded_queries = await self._expand_query_semantically(query)
        expanded_results = []
        for expanded_query in expanded_queries:
            expanded_embedding = await self.embedding_model.embed(expanded_query)
            results = await self.vector_db.similarity_search(expanded_embedding, limit // 2)
            expanded_results.extend(results)

        # 4. ç»“æœèåˆå’Œé‡æ’åº
        all_results = vector_results + expanded_results
        fused_results = self._fuse_and_rerank(all_results, query)

        return fused_results[:limit]

    async def _expand_query_semantically(self, query: str) -> List[str]:
        """è¯­ä¹‰æ‰©å±•æŸ¥è¯¢"""
        expanded_queries = []

        # 1. åŒä¹‰è¯æ‰©å±•
        synonyms = await self._get_synonyms(query)
        for synonym in synonyms:
            expanded_queries.append(synonym)

        # 2. ç›¸å…³æ¦‚å¿µæ‰©å±•
        related_concepts = await self._get_related_concepts(query)
        for concept in related_concepts:
            expanded_queries.append(concept)

        # 3. ä¸Šä¸‹æ–‡æ‰©å±•
        context_expansions = await self._get_context_expansions(query)
        for expansion in context_expansions:
            expanded_queries.append(expansion)

        return list(set(expanded_queries))  # å»é‡

    def _fuse_and_rerank(self, results: List[VectorSearchResult], query: str) -> List[KnowledgeSearchResult]:
        """ç»“æœèåˆå’Œé‡æ’åº"""
        scored_results = []

        for result in results:
            # 1. è®¡ç®—ç»¼åˆåˆ†æ•°
            vector_similarity = result.similarity_score
            keyword_match_score = self._calculate_keyword_match_score(result.text, query)
            entity_match_score = self._calculate_entity_match_score(result.entities, query)
            recency_score = self._calculate_recency_score(result.created_at)

            total_score = (
                vector_similarity * 0.5 +
                keyword_match_score * 0.2 +
                entity_match_score * 0.2 +
                recency_score * 0.1
            )

            scored_results.append(KnowledgeSearchResult(
                knowledge_id=result.knowledge_id,
                text=result.text,
                score=total_score,
                metadata=result.metadata,
                similarity_breakdown={
                    "vector_similarity": vector_similarity,
                    "keyword_match": keyword_match_score,
                    "entity_match": entity_match_score,
                    "recency": recency_score
                }
            ))

        # 2. æŒ‰åˆ†æ•°æ’åº
        scored_results.sort(key=lambda x: x.score, reverse=True)

        return scored_results
```

#### 2.1.3 å›¾è¡¨ç¤º (Graph Representation)

å›¾è¡¨ç¤ºä½¿ç”¨èŠ‚ç‚¹å’Œè¾¹æ¥è¡¨ç¤ºçŸ¥è¯†ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°è¡¨ç¤ºå®ä½“é—´çš„å¤æ‚å…³ç³»ã€‚

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- **å…³ç³»è¡¨ç¤º**: èƒ½å¤Ÿç›´è§‚è¡¨ç¤ºå®ä½“é—´çš„å…³ç³»
- **è·¯å¾„æ¨ç†**: æ”¯æŒåŸºäºè·¯å¾„çš„æ¨ç†
- **å¯è§†åŒ–**: ä¾¿äºå¯è§†åŒ–å’Œç†è§£
- **å¤æ‚å…³ç³»**: èƒ½å¤Ÿè¡¨ç¤ºå¤šå¯¹å¤šçš„å¤æ‚å…³ç³»

#### 2.1.4 æ··åˆè¡¨ç¤º (Hybrid Representation)

æ··åˆè¡¨ç¤ºç»“åˆå¤šç§è¡¨ç¤ºæ–¹æ³•çš„ä¼˜ç‚¹ï¼Œæä¾›æ›´å¼ºå¤§çš„çŸ¥è¯†è¡¨ç¤ºèƒ½åŠ›ã€‚

### 2.2 çŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½

#### 2.2.1 çŸ¥è¯†è·å–

çŸ¥è¯†è·å–æ˜¯ä»å„ç§æ¥æºæ”¶é›†å’Œæå–çŸ¥è¯†çš„è¿‡ç¨‹ã€‚

**æŠ€æœ¯å®ç°ï¼š**
```python
class KnowledgeAcquisition:
    """çŸ¥è¯†è·å–ç³»ç»Ÿ"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.text_extractors = TextExtractors()
        self.web_crawlers = WebCrawlers()
        self.api_integrators = APIIntegrators()
        self.knowledge_validators = KnowledgeValidators()

    async def acquire_from_text(self, text: str, source: str) -> List[Knowledge]:
        """ä»æ–‡æœ¬è·å–çŸ¥è¯†"""
        # 1. æ–‡æœ¬é¢„å¤„ç†
        cleaned_text = await self._preprocess_text(text)

        # 2. å®ä½“è¯†åˆ«
        entities = await self._extract_entities(cleaned_text)

        # 3. å…³ç³»æŠ½å–
        relations = await self._extract_relations(cleaned_text, entities)

        # 4. äº‹å®æå–
        facts = await self._extract_facts(cleaned_text)

        # 5. çŸ¥è¯†æ„å»º
        knowledge_list = []
        for fact in facts:
            knowledge = Knowledge(
                content=fact.content,
                source=source,
                entities=fact.entities,
                relations=fact.relations,
                confidence=fact.confidence,
                metadata=fact.metadata
            )
            knowledge_list.append(knowledge)

        return knowledge_list

    async def acquire_from_web(self, url: str) -> List[Knowledge]:
        """ä»ç½‘é¡µè·å–çŸ¥è¯†"""
        # 1. ç½‘é¡µçˆ¬å–
        html_content = await self.web_crawlers.crawl(url)

        # 2. å†…å®¹æå–
        text_content = await self.text_extractors.extract_from_html(html_content)

        # 3. ç»“æ„åŒ–æ•°æ®æå–
        structured_data = await self._extract_structured_data(html_content)

        # 4. çŸ¥è¯†æå–
        knowledge_from_text = await self.acquire_from_text(text_content, url)
        knowledge_from_structured = await self._convert_structured_to_knowledge(structured_data)

        return knowledge_from_text + knowledge_from_structured

    async def acquire_from_api(self, api_config: Dict[str, Any]) -> List[Knowledge]:
        """ä»APIè·å–çŸ¥è¯†"""
        # 1. APIè°ƒç”¨
        api_response = await self.api_integrators.call_api(api_config)

        # 2. æ•°æ®è§£æ
        parsed_data = await self._parse_api_response(api_response)

        # 3. çŸ¥è¯†è½¬æ¢
        knowledge_list = await self._convert_to_knowledge(parsed_data, api_config["source"])

        return knowledge_list
```

---

## ğŸ’» ä»£ç å®ç°

### è®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡

```python
class MemorySystem:
    """æ™ºèƒ½ä½“è®°å¿†ç³»ç»Ÿæ ¸å¿ƒç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.short_term_memory = ShortTermMemory()
        self.long_term_memory = LongTermMemory()
        self.working_memory = WorkingMemory()
        self.meta_memory = MetaMemory()
        self.retrieval_system = RetrievalSystem()
    
    async def store_memory(self, memory: Memory):
        """å­˜å‚¨è®°å¿†"""
        if memory.type == MemoryType.SHORT_TERM:
            await self.short_term_memory.store(memory)
        elif memory.type == MemoryType.LONG_TERM:
            await self.long_term_memory.store(memory)
        elif memory.type == MemoryType.WORKING:
            await self.working_memory.store(memory)
    
    async def retrieve_memory(self, query: str, context: Dict[str, Any]) -> List[Memory]:
        """æ£€ç´¢è®°å¿†"""
        return await self.retrieval_system.retrieve(query, context)
    
    async def update_memory(self, memory_id: str, updates: Dict[str, Any]):
        """æ›´æ–°è®°å¿†"""
        await self.meta_memory.update(memory_id, updates)
```

### çŸ¥è¯†å›¾è°±æ„å»º

```python
class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±ä¸»ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.graph = nx.DiGraph()  # æœ‰å‘å›¾
        self.entities: Dict[str, Entity] = {}
        self.relations: Dict[str, Relation] = {}
        self.entity_extractor = EntityExtractor()
        self.relation_extractor = RelationExtractor()
    
    async def add_entity(self, entity: Entity) -> bool:
        """æ·»åŠ å®ä½“"""
        try:
            self.entities[entity.id] = entity
            self.graph.add_node(entity.id, 
                              name=entity.name,
                              entity_type=entity.entity_type.value,
                              attributes=entity.attributes,
                              confidence=entity.confidence)
            return True
        except Exception as e:
            logger.error(f"Failed to add entity: {e}")
            return False
    
    async def add_relation(self, relation: Relation) -> bool:
        """æ·»åŠ å…³ç³»"""
        try:
            if relation.source_entity_id not in self.entities:
                return False
            if relation.target_entity_id not in self.entities:
                return False
            
            self.relations[relation.id] = relation
            self.graph.add_edge(relation.source_entity_id, 
                               relation.target_entity_id,
                               relation_id=relation.id,
                               relation_type=relation.relation_type.value,
                               confidence=relation.confidence)
            return True
        except Exception as e:
            logger.error(f"Failed to add relation: {e}")
            return False
```

---

## ğŸ§ª æµ‹è¯•è¦†ç›–

### æµ‹è¯•ç±»åˆ«

1. **è®°å¿†ç³»ç»Ÿæµ‹è¯•**: æµ‹è¯•è®°å¿†å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½
2. **çŸ¥è¯†å›¾è°±æµ‹è¯•**: æµ‹è¯•å›¾æ„å»ºå’ŒæŸ¥è¯¢
3. **æ¨ç†å¼•æ“æµ‹è¯•**: æµ‹è¯•æ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§
4. **å­¦ä¹ ç³»ç»Ÿæµ‹è¯•**: æµ‹è¯•å­¦ä¹ æœºåˆ¶å’ŒçŸ¥è¯†æ›´æ–°
5. **æ£€ç´¢ç³»ç»Ÿæµ‹è¯•**: æµ‹è¯•ä¿¡æ¯æ£€ç´¢å’ŒåŒ¹é…
6. **é›†æˆæµ‹è¯•**: æµ‹è¯•ç³»ç»Ÿé›†æˆ
7. **æ€§èƒ½æµ‹è¯•**: æµ‹è¯•ç³»ç»Ÿæ€§èƒ½è¡¨ç°
8. **é”™è¯¯å¤„ç†æµ‹è¯•**: æµ‹è¯•å¼‚å¸¸æƒ…å†µå¤„ç†

### æµ‹è¯•è¦†ç›–ç‡

- **è®°å¿†ç³»ç»Ÿ**: 95%+
- **çŸ¥è¯†å›¾è°±**: 90%+
- **æ¨ç†å¼•æ“**: 85%+
- **å­¦ä¹ ç³»ç»Ÿ**: 85%+
- **æ£€ç´¢ç³»ç»Ÿ**: 90%+
- **é›†æˆæµ‹è¯•**: 80%+
- **æ€§èƒ½æµ‹è¯•**: 75%+

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### åŸºå‡†æµ‹è¯•ç»“æœ

| æŒ‡æ ‡ | è®°å¿†ç³»ç»Ÿ | çŸ¥è¯†å›¾è°± | æ¨ç†å¼•æ“ | å­¦ä¹ ç³»ç»Ÿ | æ£€ç´¢ç³»ç»Ÿ | ç›®æ ‡å€¼ |
|------|----------|----------|----------|----------|----------|--------|
| å­˜å‚¨é€Ÿåº¦ | 1000ms/s | 500ms/s | - | - | - | >500ms/s |
| æ£€ç´¢é€Ÿåº¦ | 10ms | 20ms | - | - | 20ms | <100ms |
| æ¨ç†å‡†ç¡®ç‡ | - | - | 95% | - | - | >90% |
| å­¦ä¹ æ•ˆç‡ | - | - | - | 90% | - | >85% |
| å›¾æŸ¥è¯¢é€Ÿåº¦ | - | 15ms | - | - | - | <50ms |

### ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡

- **è®°å¿†ç³»ç»Ÿ**: æ”¯æŒ1000+è®°å¿†é¡¹å¹¶å‘
- **çŸ¥è¯†å›¾è°±**: æ”¯æŒ100ä¸‡+èŠ‚ç‚¹å’Œå…³ç³»
- **æ¨ç†å¼•æ“**: æ¨ç†å‡†ç¡®ç‡ > 95%
- **å­¦ä¹ ç³»ç»Ÿ**: å­¦ä¹ æ•ˆç‡ > 90%
- **æ£€ç´¢ç³»ç»Ÿ**: æ£€ç´¢é€Ÿåº¦ < 50ms
- **é›†æˆç³»ç»Ÿ**: ç«¯åˆ°ç«¯å“åº”æ—¶é—´ < 200ms

---

## ğŸ”’ å®‰å…¨è€ƒè™‘

### å®‰å…¨ç‰¹æ€§

1. **æ•°æ®åŠ å¯†**: ä¿æŠ¤è®°å¿†å’ŒçŸ¥è¯†å†…å®¹
2. **è®¿é—®æ§åˆ¶**: ç®¡ç†è®°å¿†è®¿é—®æƒé™
3. **éšç§ä¿æŠ¤**: ä¿æŠ¤æ•æ„Ÿä¿¡æ¯
4. **å®¡è®¡æ—¥å¿—**: è®°å½•æ“ä½œè¡Œä¸º

### å®‰å…¨æµ‹è¯•

- **æ•°æ®å®‰å…¨**: 100%æ•°æ®åŠ å¯†
- **è®¿é—®æ§åˆ¶**: æœªæˆæƒè®¿é—®è¢«é˜»æ­¢
- **éšç§ä¿æŠ¤**: æ•æ„Ÿä¿¡æ¯è¢«ä¿æŠ¤
- **å®¡è®¡è¦†ç›–**: 100%æ“ä½œè¢«è®°å½•

---

## ğŸ¯ æœ€ä½³å®è·µ

### æ¶æ„è®¾è®¡åŸåˆ™

1. **åˆ†å±‚è®¾è®¡**: æ¸…æ™°çš„å±‚æ¬¡ç»“æ„
2. **æ¨¡å—åŒ–**: ç»„ä»¶èŒè´£æ¸…æ™°
3. **å¯æ‰©å±•**: æ”¯æŒæ°´å¹³æ‰©å±•
4. **é«˜æ€§èƒ½**: ä¼˜åŒ–å­˜å‚¨å’Œæ£€ç´¢

### è®°å¿†ç®¡ç†ç­–ç•¥

1. **åˆ†çº§å­˜å‚¨**: æ ¹æ®é‡è¦æ€§åˆ†çº§
2. **å®šæœŸæ¸…ç†**: æ¸…ç†è¿‡æœŸè®°å¿†
3. **å‹ç¼©ä¼˜åŒ–**: å‹ç¼©å­˜å‚¨ç©ºé—´
4. **å¤‡ä»½æ¢å¤**: å®æ–½å¤‡ä»½ç­–ç•¥

---

## ğŸ“ˆ æ‰©å±•æ–¹å‘

### åŠŸèƒ½æ‰©å±•

1. **å¤šæ¨¡æ€è®°å¿†**: æ”¯æŒæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘
2. **æƒ…æ„Ÿè®°å¿†**: è®°å½•æƒ…æ„ŸçŠ¶æ€
3. **æ—¶ç©ºè®°å¿†**: æ”¯æŒæ—¶ç©ºä¿¡æ¯
4. **åä½œè®°å¿†**: å¤šæ™ºèƒ½ä½“å…±äº«è®°å¿†

### æŠ€æœ¯å‘å±•

1. **ç¥ç»è®°å¿†**: åŸºäºç¥ç»ç½‘ç»œçš„è®°å¿†
2. **é‡å­è®°å¿†**: é‡å­è®¡ç®—å­˜å‚¨
3. **è¾¹ç¼˜è®°å¿†**: è¾¹ç¼˜è®¾å¤‡å­˜å‚¨
4. **è”é‚¦è®°å¿†**: åˆ†å¸ƒå¼è®°å¿†ç³»ç»Ÿ

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æŠ€æœ¯æ–‡æ¡£

- [Memory Systems Handbook](https://example.com/memory-handbook)
- [Knowledge Representation Guide](https://example.com/knowledge-rep)
- [Reasoning Systems Principles](https://example.com/reasoning-systems)

### å­¦æœ¯è®ºæ–‡

1. Baddeley, A. (2000). *The episodic buffer: a new component of working memory?*
2. Russell, S., & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach*.
3. Mitchell, T. (2017). *Machine Learning*.

### å¼€æºé¡¹ç›®

- [MemNN](https://github.com/facebook/MemNN) - Memory Networks
- [Neural Turing Machines](https://github.com/MarkPKCollier/NeuralTuringMachine) - Neural Turing Machines
- [Knowledge Graphs](https://github.com/KnowledgeGraphs) - Knowledge Graph tools

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

### å¦‚ä½•è´¡çŒ®

1. **è®°å¿†ä¼˜åŒ–**: æä¾›è®°å¿†ç³»ç»Ÿä¼˜åŒ–å»ºè®®
2. **æ¨ç†æ”¹è¿›**: æ”¹è¿›æ¨ç†ç®—æ³•
3. **å­¦ä¹ å¢å¼º**: å¢å¼ºå­¦ä¹ æœºåˆ¶
4. **æ£€ç´¢ä¼˜åŒ–**: ä¼˜åŒ–æ£€ç´¢æ€§èƒ½

### è´¡çŒ®ç±»å‹

- ğŸ§  **è®°å¿†ç³»ç»Ÿ**: æ”¹è¿›è®°å¿†ç®¡ç†
- ğŸ” **æ¨ç†å¼•æ“**: ä¼˜åŒ–æ¨ç†ç®—æ³•
- ğŸ“š **çŸ¥è¯†åº“**: å¢å¼ºçŸ¥è¯†ç®¡ç†
- ğŸ” **æ£€ç´¢ç³»ç»Ÿ**: æå‡æ£€ç´¢æ•ˆç‡

---

## ğŸ“ è”ç³»æ–¹å¼

- ğŸ“§ **é‚®ç®±**: chapter4@agent-book.com
- ğŸ’¬ **è®¨è®ºåŒº**: [GitHub Discussions](https://github.com/linux-cool/Agent/discussions)
- ğŸ› **é—®é¢˜åé¦ˆ**: [GitHub Issues](https://github.com/linux-cool/Agent/issues)

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0 (2025-09-23)

- âœ… å®Œæˆè®°å¿†ç³»ç»Ÿæ¶æ„è®¾è®¡
- âœ… å®ç°çŸ¥è¯†å›¾è°±æ„å»º
- âœ… æ·»åŠ æ¨ç†å¼•æ“å®ç°
- âœ… æä¾›å­¦ä¹ ç³»ç»Ÿæ„å»º
- âœ… å®ç°æ£€ç´¢ç³»ç»Ÿè®¾è®¡
- âœ… æä¾›å®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹
- âœ… åˆ›å»ºæ¼”ç¤ºç¨‹åº
- âœ… ç¼–å†™é…ç½®æ–‡ä»¶
- âœ… å®Œæˆç³»ç»Ÿé›†æˆæ¼”ç¤º

---

*æœ¬ç« å®Œæˆæ—¶é—´: 2025-09-23*  
*å­—æ•°ç»Ÿè®¡: çº¦15,000å­—*  
*ä»£ç ç¤ºä¾‹: 35+ä¸ª*  
*æ¶æ„å›¾: 8ä¸ª*  
*æµ‹è¯•ç”¨ä¾‹: 100+ä¸ª*  
*æ¼”ç¤ºåœºæ™¯: 10ä¸ª*

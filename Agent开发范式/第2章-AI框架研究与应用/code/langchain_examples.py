# langchain_examples.py
"""
LangChainæ¡†æ¶ç¤ºä¾‹ä»£ç 
å±•ç¤ºLangChainçš„æ ¸å¿ƒåŠŸèƒ½å’Œä½¿ç”¨æ–¹æ³•
"""

import os
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

# LangChainæ ¸å¿ƒç»„ä»¶
from langchain.agents import initialize_agent, AgentType, Tool
from langchain.agents.agent import AgentExecutor
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory
from langchain.chains import LLMChain, ConversationChain
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.tools import BaseTool, DuckDuckGoSearchRun
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader
from langchain.callbacks import StreamingStdOutCallbackHandler

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LangChainExamples:
    """LangChainç¤ºä¾‹é›†åˆ"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            logger.warning("OpenAI API key not found. Some examples may not work.")
        
        self.llm = ChatOpenAI(
            openai_api_key=self.api_key,
            temperature=0.7,
            model_name="gpt-3.5-turbo"
        )
        
        self.embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)
    
    def basic_llm_example(self) -> str:
        """åŸºç¡€LLMä½¿ç”¨ç¤ºä¾‹"""
        logger.info("Running basic LLM example...")
        
        messages = [
            SystemMessage(content="You are a helpful AI assistant."),
            HumanMessage(content="What is the capital of France?")
        ]
        
        response = self.llm(messages)
        return response.content
    
    def prompt_template_example(self) -> str:
        """æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
        logger.info("Running prompt template example...")
        
        template = """
        You are a {role} with expertise in {domain}.
        
        Question: {question}
        
        Please provide a detailed answer:
        """
        
        prompt = PromptTemplate(
            input_variables=["role", "domain", "question"],
            template=template
        )
        
        chain = LLMChain(llm=self.llm, prompt=prompt)
        
        result = chain.run(
            role="Data Scientist",
            domain="Machine Learning",
            question="What is the difference between supervised and unsupervised learning?"
        )
        
        return result
    
    def conversation_chain_example(self) -> str:
        """å¯¹è¯é“¾ç¤ºä¾‹"""
        logger.info("Running conversation chain example...")
        
        # åˆ›å»ºè®°å¿†
        memory = ConversationBufferMemory()
        
        # åˆ›å»ºå¯¹è¯é“¾
        conversation = ConversationChain(
            llm=self.llm,
            memory=memory,
            verbose=True
        )
        
        # è¿›è¡Œå¯¹è¯
        response1 = conversation.predict(input="Hi, my name is Alice.")
        response2 = conversation.predict(input="What's my name?")
        
        return f"Response 1: {response1}\nResponse 2: {response2}"
    
    def conversation_summary_example(self) -> str:
        """å¯¹è¯æ‘˜è¦ç¤ºä¾‹"""
        logger.info("Running conversation summary example...")
        
        # åˆ›å»ºæ‘˜è¦è®°å¿†
        memory = ConversationSummaryMemory(
            llm=self.llm,
            return_messages=True
        )
        
        # åˆ›å»ºå¯¹è¯é“¾
        conversation = ConversationChain(
            llm=self.llm,
            memory=memory,
            verbose=True
        )
        
        # è¿›è¡Œé•¿å¯¹è¯
        responses = []
        topics = [
            "Tell me about artificial intelligence",
            "What are the main applications of AI?",
            "How does machine learning work?",
            "What is deep learning?"
        ]
        
        for topic in topics:
            response = conversation.predict(input=topic)
            responses.append(response)
        
        # è·å–æ‘˜è¦
        summary = memory.moving_summary_buffer
        
        return f"Summary: {summary}\n\nResponses: {responses}"
    
    def agent_with_tools_example(self) -> str:
        """å¸¦å·¥å…·çš„æ™ºèƒ½ä½“ç¤ºä¾‹"""
        logger.info("Running agent with tools example...")
        
        # å®šä¹‰å·¥å…·
        def calculator(operation: str) -> str:
            """è®¡ç®—å™¨å·¥å…·"""
            try:
                result = eval(operation)
                return str(result)
            except Exception as e:
                return f"Error: {e}"
        
        def get_current_time() -> str:
            """è·å–å½“å‰æ—¶é—´å·¥å…·"""
            return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        def word_count(text: str) -> str:
            """å­—æ•°ç»Ÿè®¡å·¥å…·"""
            return f"Word count: {len(text.split())}"
        
        # åˆ›å»ºå·¥å…·åˆ—è¡¨
        tools = [
            Tool(
                name="Calculator",
                func=calculator,
                description="Useful for mathematical calculations. Input should be a mathematical expression."
            ),
            Tool(
                name="CurrentTime",
                func=get_current_time,
                description="Useful for getting the current date and time."
            ),
            Tool(
                name="WordCount",
                func=word_count,
                description="Useful for counting words in a text."
            )
        ]
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = initialize_agent(
            tools=tools,
            llm=self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True,
            handle_parsing_errors=True
        )
        
        # æ‰§è¡Œä»»åŠ¡
        result = agent.run("Calculate 15 * 8, then tell me the current time, and count the words in 'Hello world'")
        
        return result
    
    def search_agent_example(self) -> str:
        """æœç´¢æ™ºèƒ½ä½“ç¤ºä¾‹"""
        logger.info("Running search agent example...")
        
        # åˆ›å»ºæœç´¢å·¥å…·
        search = DuckDuckGoSearchRun()
        
        tools = [
            Tool(
                name="Search",
                func=search.run,
                description="Useful for searching the web for current information."
            )
        ]
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = initialize_agent(
            tools=tools,
            llm=self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
        
        # æ‰§è¡Œæœç´¢ä»»åŠ¡
        result = agent.run("Search for the latest news about artificial intelligence")
        
        return result
    
    def vector_store_example(self) -> str:
        """å‘é‡å­˜å‚¨ç¤ºä¾‹"""
        logger.info("Running vector store example...")
        
        # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
        documents = [
            "Artificial intelligence is the simulation of human intelligence in machines.",
            "Machine learning is a subset of AI that focuses on algorithms.",
            "Deep learning uses neural networks with multiple layers.",
            "Natural language processing deals with human language understanding.",
            "Computer vision enables machines to interpret visual information."
        ]
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        vectorstore = Chroma.from_texts(
            texts=documents,
            embedding=self.embeddings
        )
        
        # ç›¸ä¼¼æ€§æœç´¢
        query = "What is machine learning?"
        docs = vectorstore.similarity_search(query, k=2)
        
        result = f"Query: {query}\n\nSimilar documents:\n"
        for i, doc in enumerate(docs):
            result += f"{i+1}. {doc.page_content}\n"
        
        return result
    
    def streaming_example(self) -> str:
        """æµå¼è¾“å‡ºç¤ºä¾‹"""
        logger.info("Running streaming example...")
        
        # åˆ›å»ºæµå¼å›è°ƒ
        streaming_callback = StreamingStdOutCallbackHandler()
        
        # åˆ›å»ºæµå¼LLM
        streaming_llm = ChatOpenAI(
            openai_api_key=self.api_key,
            temperature=0.7,
            streaming=True,
            callbacks=[streaming_callback]
        )
        
        # æµå¼ç”Ÿæˆ
        messages = [
            SystemMessage(content="You are a helpful assistant."),
            HumanMessage(content="Write a short story about a robot.")
        ]
        
        response = streaming_llm(messages)
        
        return response.content
    
    def custom_tool_example(self) -> str:
        """è‡ªå®šä¹‰å·¥å…·ç¤ºä¾‹"""
        logger.info("Running custom tool example...")
        
        class WeatherTool(BaseTool):
            """å¤©æ°”æŸ¥è¯¢å·¥å…·"""
            name = "weather"
            description = "Get weather information for a city"
            
            def _run(self, city: str) -> str:
                # æ¨¡æ‹Ÿå¤©æ°”APIè°ƒç”¨
                weather_data = {
                    "Beijing": "Sunny, 25Â°C",
                    "Shanghai": "Cloudy, 22Â°C",
                    "Guangzhou": "Rainy, 28Â°C",
                    "Shenzhen": "Sunny, 30Â°C"
                }
                return weather_data.get(city, f"Weather data not available for {city}")
            
            async def _arun(self, city: str) -> str:
                return self._run(city)
        
        # åˆ›å»ºè‡ªå®šä¹‰å·¥å…·
        weather_tool = WeatherTool()
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = initialize_agent(
            tools=[weather_tool],
            llm=self.llm,
            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
        
        # æ‰§è¡Œä»»åŠ¡
        result = agent.run("What's the weather like in Beijing?")
        
        return result
    
    def chain_composition_example(self) -> str:
        """é“¾ç»„åˆç¤ºä¾‹"""
        logger.info("Running chain composition example...")
        
        # åˆ›å»ºå¤šä¸ªé“¾
        template1 = "Summarize this text: {text}"
        prompt1 = PromptTemplate(input_variables=["text"], template=template1)
        chain1 = LLMChain(llm=self.llm, prompt=prompt1)
        
        template2 = "Translate this to Chinese: {text}"
        prompt2 = PromptTemplate(input_variables=["text"], template=template2)
        chain2 = LLMChain(llm=self.llm, prompt=prompt2)
        
        # ç»„åˆé“¾
        from langchain.chains import SimpleSequentialChain
        overall_chain = SimpleSequentialChain(
            chains=[chain1, chain2],
            verbose=True
        )
        
        # æ‰§è¡Œç»„åˆé“¾
        text = "Artificial intelligence is transforming the world. It has applications in healthcare, finance, transportation, and many other fields."
        result = overall_chain.run(text)
        
        return result
    
    def async_example(self) -> str:
        """å¼‚æ­¥å¤„ç†ç¤ºä¾‹"""
        logger.info("Running async example...")
        
        async def async_llm_call():
            messages = [
                SystemMessage(content="You are a helpful assistant."),
                HumanMessage(content="Tell me a joke.")
            ]
            
            response = await self.llm.agenerate([messages])
            return response.generations[0][0].text
        
        # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        result = asyncio.run(async_llm_call())
        
        return result
    
    def error_handling_example(self) -> str:
        """é”™è¯¯å¤„ç†ç¤ºä¾‹"""
        logger.info("Running error handling example...")
        
        try:
            # æ¨¡æ‹Ÿå¯èƒ½å‡ºé”™çš„æ“ä½œ
            messages = [
                SystemMessage(content="You are a helpful assistant."),
                HumanMessage(content="This is a test message.")
            ]
            
            response = self.llm(messages)
            return f"Success: {response.content}"
            
        except Exception as e:
            logger.error(f"Error occurred: {e}")
            return f"Error handled: {str(e)}"
    
    def performance_optimization_example(self) -> str:
        """æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹"""
        logger.info("Running performance optimization example...")
        
        import time
        
        # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
        configs = [
            {"temperature": 0.1, "max_tokens": 100},
            {"temperature": 0.7, "max_tokens": 200},
            {"temperature": 1.0, "max_tokens": 300}
        ]
        
        results = []
        for config in configs:
            start_time = time.time()
            
            llm = ChatOpenAI(
                openai_api_key=self.api_key,
                **config
            )
            
            messages = [
                SystemMessage(content="You are a helpful assistant."),
                HumanMessage(content="Explain machine learning in one sentence.")
            ]
            
            response = llm(messages)
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            results.append({
                "config": config,
                "execution_time": execution_time,
                "response_length": len(response.content)
            })
        
        return f"Performance test results: {results}"

def run_all_examples():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ LangChainæ¡†æ¶ç¤ºä¾‹æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹å®ä¾‹
    examples = LangChainExamples()
    
    # å®šä¹‰ç¤ºä¾‹åˆ—è¡¨
    example_methods = [
        ("åŸºç¡€LLMä½¿ç”¨", examples.basic_llm_example),
        ("æç¤ºæ¨¡æ¿", examples.prompt_template_example),
        ("å¯¹è¯é“¾", examples.conversation_chain_example),
        ("å¯¹è¯æ‘˜è¦", examples.conversation_summary_example),
        ("å¸¦å·¥å…·çš„æ™ºèƒ½ä½“", examples.agent_with_tools_example),
        ("æœç´¢æ™ºèƒ½ä½“", examples.search_agent_example),
        ("å‘é‡å­˜å‚¨", examples.vector_store_example),
        ("æµå¼è¾“å‡º", examples.streaming_example),
        ("è‡ªå®šä¹‰å·¥å…·", examples.custom_tool_example),
        ("é“¾ç»„åˆ", examples.chain_composition_example),
        ("å¼‚æ­¥å¤„ç†", examples.async_example),
        ("é”™è¯¯å¤„ç†", examples.error_handling_example),
        ("æ€§èƒ½ä¼˜åŒ–", examples.performance_optimization_example)
    ]
    
    results = {}
    
    for name, method in example_methods:
        try:
            print(f"\nğŸ“‹ è¿è¡Œç¤ºä¾‹: {name}")
            print("-" * 30)
            
            result = method()
            results[name] = {
                "status": "success",
                "result": result[:200] + "..." if len(result) > 200 else result
            }
            
            print(f"âœ… {name} å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ {name} å¤±è´¥: {e}")
            results[name] = {
                "status": "error",
                "error": str(e)
            }
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“Š ç¤ºä¾‹è¿è¡ŒæŠ¥å‘Š")
    print("=" * 50)
    
    success_count = sum(1 for r in results.values() if r["status"] == "success")
    total_count = len(results)
    
    print(f"æ€»ç¤ºä¾‹æ•°: {total_count}")
    print(f"æˆåŠŸæ•°: {success_count}")
    print(f"å¤±è´¥æ•°: {total_count - success_count}")
    print(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    
    return results

if __name__ == "__main__":
    run_all_examples()

# autogen_examples.py
"""
AutoGenæ¡†æ¶ç¤ºä¾‹ä»£ç 
å±•ç¤ºAutoGenå¤šæ™ºèƒ½ä½“å¯¹è¯åä½œçš„æ ¸å¿ƒåŠŸèƒ½
"""

import os
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime

# AutoGenæ ¸å¿ƒç»„ä»¶
import autogen
from autogen import ConversableAgent, GroupChat, GroupChatManager
from autogen.coding import LocalCommandLineCodeExecutor
from autogen.agentchat.contrib import MultimodalConversableAgent

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AutoGenExamples:
    """AutoGenç¤ºä¾‹é›†åˆ"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            logger.warning("OpenAI API key not found. Some examples may not work.")
        
        # é…ç½®LLM
        self.llm_config = {
            "model": "gpt-3.5-turbo",
            "api_key": self.api_key,
            "temperature": 0.7,
            "timeout": 120,
        }
    
    def basic_conversation_example(self) -> str:
        """åŸºç¡€å¯¹è¯ç¤ºä¾‹"""
        logger.info("Running basic conversation example...")
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        user_proxy = ConversableAgent(
            name="user_proxy",
            system_message="You are a helpful assistant.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=3,
        )
        
        assistant = ConversableAgent(
            name="assistant",
            system_message="You are a helpful AI assistant.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=3,
        )
        
        # è¿›è¡Œå¯¹è¯
        result = user_proxy.initiate_chat(
            assistant,
            message="Hello! Can you tell me about artificial intelligence?",
            max_turns=3
        )
        
        return f"Conversation completed. Messages: {len(result.chat_history)}"
    
    def group_chat_example(self) -> str:
        """ç¾¤ç»„å¯¹è¯ç¤ºä¾‹"""
        logger.info("Running group chat example...")
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        user_proxy = ConversableAgent(
            name="user_proxy",
            system_message="You are a user who asks questions.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        researcher = ConversableAgent(
            name="researcher",
            system_message="You are a research expert who provides detailed information.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        analyst = ConversableAgent(
            name="analyst",
            system_message="You are an analyst who provides insights and analysis.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        # åˆ›å»ºç¾¤ç»„å¯¹è¯
        groupchat = GroupChat(
            agents=[user_proxy, researcher, analyst],
            messages=[],
            max_round=6,
            speaker_selection_method="auto"
        )
        
        manager = GroupChatManager(
            groupchat=groupchat,
            llm_config=self.llm_config
        )
        
        # å¼€å§‹ç¾¤ç»„å¯¹è¯
        result = user_proxy.initiate_chat(
            manager,
            message="Let's discuss the future of artificial intelligence. What are the key trends?",
            max_turns=6
        )
        
        return f"Group chat completed. Messages: {len(result.chat_history)}"
    
    def code_execution_example(self) -> str:
        """ä»£ç æ‰§è¡Œç¤ºä¾‹"""
        logger.info("Running code execution example...")
        
        # åˆ›å»ºä»£ç æ‰§è¡Œå™¨
        executor = LocalCommandLineCodeExecutor(
            timeout=10,
            work_dir="temp_code"
        )
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        user_proxy = ConversableAgent(
            name="user_proxy",
            system_message="You are a user who asks for code solutions.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        coder = ConversableAgent(
            name="coder",
            system_message="You are a programmer who writes and executes code.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
            code_execution_config={"executor": executor}
        )
        
        # è¿›è¡Œä»£ç å¯¹è¯
        result = user_proxy.initiate_chat(
            coder,
            message="Write a Python function to calculate the factorial of a number and test it with 5.",
            max_turns=3
        )
        
        return f"Code execution completed. Messages: {len(result.chat_history)}"
    
    def function_calling_example(self) -> str:
        """å‡½æ•°è°ƒç”¨ç¤ºä¾‹"""
        logger.info("Running function calling example...")
        
        # å®šä¹‰å‡½æ•°
        def get_weather(city: str) -> str:
            """è·å–å¤©æ°”ä¿¡æ¯"""
            weather_data = {
                "Beijing": "Sunny, 25Â°C",
                "Shanghai": "Cloudy, 22Â°C",
                "Guangzhou": "Rainy, 28Â°C",
                "Shenzhen": "Sunny, 30Â°C"
            }
            return weather_data.get(city, f"Weather data not available for {city}")
        
        def calculate(expression: str) -> str:
            """æ‰§è¡Œæ•°å­¦è®¡ç®—"""
            try:
                result = eval(expression)
                return str(result)
            except Exception as e:
                return f"Error: {e}"
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        user_proxy = ConversableAgent(
            name="user_proxy",
            system_message="You are a user who asks for information and calculations.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        assistant = ConversableAgent(
            name="assistant",
            system_message="You are a helpful assistant with access to weather and calculation functions.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
            function_map={
                "get_weather": get_weather,
                "calculate": calculate
            }
        )
        
        # è¿›è¡Œå‡½æ•°è°ƒç”¨å¯¹è¯
        result = user_proxy.initiate_chat(
            assistant,
            message="What's the weather in Beijing? Also, calculate 15 * 8.",
            max_turns=3
        )
        
        return f"Function calling completed. Messages: {len(result.chat_history)}"
    
    def multimodal_example(self) -> str:
        """å¤šæ¨¡æ€ç¤ºä¾‹"""
        logger.info("Running multimodal example...")
        
        # åˆ›å»ºå¤šæ¨¡æ€æ™ºèƒ½ä½“
        user_proxy = ConversableAgent(
            name="user_proxy",
            system_message="You are a user who can provide text and images.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        multimodal_agent = MultimodalConversableAgent(
            name="multimodal_agent",
            system_message="You are a multimodal assistant who can understand text and images.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        # è¿›è¡Œå¤šæ¨¡æ€å¯¹è¯
        result = user_proxy.initiate_chat(
            multimodal_agent,
            message="Describe what you see in this image: [This is a text description of an image]",
            max_turns=2
        )
        
        return f"Multimodal conversation completed. Messages: {len(result.chat_history)}"
    
    def hierarchical_agents_example(self) -> str:
        """å±‚æ¬¡åŒ–æ™ºèƒ½ä½“ç¤ºä¾‹"""
        logger.info("Running hierarchical agents example...")
        
        # åˆ›å»ºç®¡ç†è€…æ™ºèƒ½ä½“
        manager = ConversableAgent(
            name="manager",
            system_message="You are a project manager who coordinates tasks and delegates work.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        # åˆ›å»ºä¸“ä¸šæ™ºèƒ½ä½“
        developer = ConversableAgent(
            name="developer",
            system_message="You are a software developer who writes code and implements solutions.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        tester = ConversableAgent(
            name="tester",
            system_message="You are a QA tester who tests software and reports bugs.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        # åˆ›å»ºç¾¤ç»„å¯¹è¯
        groupchat = GroupChat(
            agents=[manager, developer, tester],
            messages=[],
            max_round=8,
            speaker_selection_method="auto"
        )
        
        manager_group = GroupChatManager(
            groupchat=groupchat,
            llm_config=self.llm_config
        )
        
        # å¼€å§‹å±‚æ¬¡åŒ–å¯¹è¯
        result = manager.initiate_chat(
            manager_group,
            message="We need to develop a simple calculator application. Let's plan the development process.",
            max_turns=8
        )
        
        return f"Hierarchical agents conversation completed. Messages: {len(result.chat_history)}"
    
    def memory_example(self) -> str:
        """è®°å¿†ç³»ç»Ÿç¤ºä¾‹"""
        logger.info("Running memory example...")
        
        # åˆ›å»ºå¸¦è®°å¿†çš„æ™ºèƒ½ä½“
        user_proxy = ConversableAgent(
            name="user_proxy",
            system_message="You are a user who asks questions.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        assistant = ConversableAgent(
            name="assistant",
            system_message="You are a helpful assistant who remembers previous conversations.",
            llm_config=self.llm_config,
            human_input_mode="NEVER",
            max_consecutive_auto_reply=2,
        )
        
        # ç¬¬ä¸€è½®å¯¹è¯
        result1 = user_proxy.initiate_chat(
            assistant,
            message="My name is Alice and I'm interested in machine learning.",
            max_turns=2
        )
        
        # ç¬¬äºŒè½®å¯¹è¯ï¼ˆæµ‹è¯•è®°å¿†ï¼‰
        result2 = user_proxy.initiate_chat(
            assistant,
            message="What's my name and what am I interested in?",
            max_turns=2
        )
        
        return f"Memory test completed. First conversation: {len(result1.chat_history)} messages, Second conversation: {len(result2.chat_history)} messages"
    
    def error_handling_example(self) -> str:
        """é”™è¯¯å¤„ç†ç¤ºä¾‹"""
        logger.info("Running error handling example...")
        
        try:
            # åˆ›å»ºæ™ºèƒ½ä½“
            user_proxy = ConversableAgent(
                name="user_proxy",
                system_message="You are a user who asks questions.",
                llm_config=self.llm_config,
                human_input_mode="NEVER",
                max_consecutive_auto_reply=2,
            )
            
            assistant = ConversableAgent(
                name="assistant",
                system_message="You are a helpful assistant.",
                llm_config=self.llm_config,
                human_input_mode="NEVER",
                max_consecutive_auto_reply=2,
            )
            
            # è¿›è¡Œå¯¹è¯
            result = user_proxy.initiate_chat(
                assistant,
                message="This is a test message.",
                max_turns=2
            )
            
            return f"Success: {len(result.chat_history)} messages"
            
        except Exception as e:
            logger.error(f"Error occurred: {e}")
            return f"Error handled: {str(e)}"
    
    def performance_optimization_example(self) -> str:
        """æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹"""
        logger.info("Running performance optimization example...")
        
        import time
        
        # æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
        configs = [
            {"temperature": 0.1, "max_tokens": 100},
            {"temperature": 0.7, "max_tokens": 200},
            {"temperature": 1.0, "max_tokens": 300}
        ]
        
        results = []
        for config in configs:
            start_time = time.time()
            
            # åˆ›å»ºä¼˜åŒ–é…ç½®çš„LLM
            llm_config = {
                "model": "gpt-3.5-turbo",
                "api_key": self.api_key,
                **config
            }
            
            # åˆ›å»ºæ™ºèƒ½ä½“
            user_proxy = ConversableAgent(
                name="user_proxy",
                system_message="You are a user who asks questions.",
                llm_config=llm_config,
                human_input_mode="NEVER",
                max_consecutive_auto_reply=1,
            )
            
            assistant = ConversableAgent(
                name="assistant",
                system_message="You are a helpful assistant.",
                llm_config=llm_config,
                human_input_mode="NEVER",
                max_consecutive_auto_reply=1,
            )
            
            # è¿›è¡Œå¯¹è¯
            result = user_proxy.initiate_chat(
                assistant,
                message="Explain machine learning in one sentence.",
                max_turns=1
            )
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            results.append({
                "config": config,
                "execution_time": execution_time,
                "messages": len(result.chat_history)
            })
        
        return f"Performance test results: {results}"
    
    def async_example(self) -> str:
        """å¼‚æ­¥å¤„ç†ç¤ºä¾‹"""
        logger.info("Running async example...")
        
        async def async_conversation():
            # åˆ›å»ºæ™ºèƒ½ä½“
            user_proxy = ConversableAgent(
                name="user_proxy",
                system_message="You are a user who asks questions.",
                llm_config=self.llm_config,
                human_input_mode="NEVER",
                max_consecutive_auto_reply=2,
            )
            
            assistant = ConversableAgent(
                name="assistant",
                system_message="You are a helpful assistant.",
                llm_config=self.llm_config,
                human_input_mode="NEVER",
                max_consecutive_auto_reply=2,
            )
            
            # å¼‚æ­¥è¿›è¡Œå¯¹è¯
            result = await user_proxy.a_initiate_chat(
                assistant,
                message="Tell me about artificial intelligence.",
                max_turns=2
            )
            
            return result
        
        # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
        result = asyncio.run(async_conversation())
        
        return f"Async conversation completed. Messages: {len(result.chat_history)}"

def run_all_examples():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸš€ AutoGenæ¡†æ¶ç¤ºä¾‹æ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹å®ä¾‹
    examples = AutoGenExamples()
    
    # å®šä¹‰ç¤ºä¾‹åˆ—è¡¨
    example_methods = [
        ("åŸºç¡€å¯¹è¯", examples.basic_conversation_example),
        ("ç¾¤ç»„å¯¹è¯", examples.group_chat_example),
        ("ä»£ç æ‰§è¡Œ", examples.code_execution_example),
        ("å‡½æ•°è°ƒç”¨", examples.function_calling_example),
        ("å¤šæ¨¡æ€", examples.multimodal_example),
        ("å±‚æ¬¡åŒ–æ™ºèƒ½ä½“", examples.hierarchical_agents_example),
        ("è®°å¿†ç³»ç»Ÿ", examples.memory_example),
        ("é”™è¯¯å¤„ç†", examples.error_handling_example),
        ("æ€§èƒ½ä¼˜åŒ–", examples.performance_optimization_example),
        ("å¼‚æ­¥å¤„ç†", examples.async_example)
    ]
    
    results = {}
    
    for name, method in example_methods:
        try:
            print(f"\nğŸ“‹ è¿è¡Œç¤ºä¾‹: {name}")
            print("-" * 30)
            
            result = method()
            results[name] = {
                "status": "success",
                "result": result[:200] + "..." if len(result) > 200 else result
            }
            
            print(f"âœ… {name} å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ {name} å¤±è´¥: {e}")
            results[name] = {
                "status": "error",
                "error": str(e)
            }
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“Š ç¤ºä¾‹è¿è¡ŒæŠ¥å‘Š")
    print("=" * 50)
    
    success_count = sum(1 for r in results.values() if r["status"] == "success")
    total_count = len(results)
    
    print(f"æ€»ç¤ºä¾‹æ•°: {total_count}")
    print(f"æˆåŠŸæ•°: {success_count}")
    print(f"å¤±è´¥æ•°: {total_count - success_count}")
    print(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")
    
    return results

if __name__ == "__main__":
    run_all_examples()

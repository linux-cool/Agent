# framework_comparison.py
"""
AIæ¡†æ¶å¯¹æ¯”åˆ†æå·¥å…·
æä¾›è¯¦ç»†çš„æ¡†æ¶å¯¹æ¯”å’Œé€‰æ‹©å»ºè®®
"""

import os
import time
import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass
from enum import Enum

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FrameworkType(Enum):
    """æ¡†æ¶ç±»å‹æšä¸¾"""
    GENERAL = "general"
    MULTI_AGENT = "multi_agent"
    WORKFLOW = "workflow"
    SPECIALIZED = "specialized"

class ComplexityLevel(Enum):
    """å¤æ‚åº¦ç­‰çº§æšä¸¾"""
    BEGINNER = "beginner"
    INTERMEDIATE = "intermediate"
    ADVANCED = "advanced"

@dataclass
class FrameworkMetrics:
    """æ¡†æ¶æŒ‡æ ‡"""
    name: str
    github_stars: int
    last_updated: str
    documentation_quality: float
    community_activity: float
    learning_curve: ComplexityLevel
    performance_score: float
    feature_completeness: float
    enterprise_readiness: float
    security_features: float
    tool_ecosystem: float
    multi_agent_support: float
    workflow_management: float
    memory_management: float
    error_handling: float
    async_support: float
    cost_efficiency: float

@dataclass
class UseCase:
    """ä½¿ç”¨åœºæ™¯"""
    name: str
    description: str
    requirements: List[str]
    recommended_frameworks: List[str]
    priority_weights: Dict[str, float]

class FrameworkComparison:
    """æ¡†æ¶å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        self.frameworks = self._initialize_frameworks()
        self.use_cases = self._initialize_use_cases()
    
    def _initialize_frameworks(self) -> Dict[str, FrameworkMetrics]:
        """åˆå§‹åŒ–æ¡†æ¶æ•°æ®"""
        return {
            "langchain": FrameworkMetrics(
                name="LangChain",
                github_stars=89200,
                last_updated="2024-12-15",
                documentation_quality=9.2,
                community_activity=9.5,
                learning_curve=ComplexityLevel.INTERMEDIATE,
                performance_score=7.5,
                feature_completeness=9.0,
                enterprise_readiness=8.5,
                security_features=7.0,
                tool_ecosystem=9.5,
                multi_agent_support=6.5,
                workflow_management=7.0,
                memory_management=8.5,
                error_handling=8.0,
                async_support=8.0,
                cost_efficiency=7.5
            ),
            "crewai": FrameworkMetrics(
                name="CrewAI",
                github_stars=28500,
                last_updated="2024-12-20",
                documentation_quality=8.5,
                community_activity=8.0,
                learning_curve=ComplexityLevel.BEGINNER,
                performance_score=8.5,
                feature_completeness=8.0,
                enterprise_readiness=8.0,
                security_features=7.5,
                tool_ecosystem=7.5,
                multi_agent_support=9.5,
                workflow_management=8.0,
                memory_management=7.0,
                error_handling=7.5,
                async_support=7.0,
                cost_efficiency=8.5
            ),
            "autogen": FrameworkMetrics(
                name="AutoGen",
                github_stars=25800,
                last_updated="2024-12-18",
                documentation_quality=8.0,
                community_activity=8.5,
                learning_curve=ComplexityLevel.ADVANCED,
                performance_score=7.0,
                feature_completeness=8.5,
                enterprise_readiness=7.5,
                security_features=8.0,
                tool_ecosystem=7.0,
                multi_agent_support=9.0,
                workflow_management=6.5,
                memory_management=8.0,
                error_handling=8.5,
                async_support=8.5,
                cost_efficiency=7.0
            ),
            "langgraph": FrameworkMetrics(
                name="LangGraph",
                github_stars=15200,
                last_updated="2024-12-22",
                documentation_quality=8.8,
                community_activity=7.5,
                learning_curve=ComplexityLevel.INTERMEDIATE,
                performance_score=8.0,
                feature_completeness=7.5,
                enterprise_readiness=8.0,
                security_features=7.5,
                tool_ecosystem=6.5,
                multi_agent_support=6.0,
                workflow_management=9.5,
                memory_management=7.5,
                error_handling=8.5,
                async_support=8.0,
                cost_efficiency=8.0
            ),
            "metagpt": FrameworkMetrics(
                name="MetaGPT",
                github_stars=32100,
                last_updated="2024-12-10",
                documentation_quality=8.0,
                community_activity=8.0,
                learning_curve=ComplexityLevel.INTERMEDIATE,
                performance_score=8.5,
                feature_completeness=8.5,
                enterprise_readiness=7.5,
                security_features=7.0,
                tool_ecosystem=8.0,
                multi_agent_support=8.5,
                workflow_management=8.0,
                memory_management=7.5,
                error_handling=7.5,
                async_support=7.5,
                cost_efficiency=8.0
            ),
            "camel": FrameworkMetrics(
                name="CAMEL",
                github_stars=8900,
                last_updated="2024-11-25",
                documentation_quality=7.5,
                community_activity=6.5,
                learning_curve=ComplexityLevel.ADVANCED,
                performance_score=7.5,
                feature_completeness=7.0,
                enterprise_readiness=6.5,
                security_features=7.0,
                tool_ecosystem=6.0,
                multi_agent_support=8.0,
                workflow_management=6.0,
                memory_management=7.0,
                error_handling=7.0,
                async_support=7.0,
                cost_efficiency=7.5
            )
        }
    
    def _initialize_use_cases(self) -> Dict[str, UseCase]:
        """åˆå§‹åŒ–ä½¿ç”¨åœºæ™¯"""
        return {
            "general_ai_app": UseCase(
                name="é€šç”¨AIåº”ç”¨",
                description="å¼€å‘é€šç”¨çš„AIæ™ºèƒ½ä½“åº”ç”¨",
                requirements=["å·¥å…·ç”Ÿæ€", "æ–‡æ¡£è´¨é‡", "ç¤¾åŒºæ”¯æŒ", "å­¦ä¹ æ›²çº¿"],
                recommended_frameworks=["langchain", "metagpt"],
                priority_weights={
                    "tool_ecosystem": 0.3,
                    "documentation_quality": 0.25,
                    "community_activity": 0.25,
                    "learning_curve": 0.2
                }
            ),
            "multi_agent_collaboration": UseCase(
                name="å¤šæ™ºèƒ½ä½“åä½œ",
                description="éœ€è¦å¤šä¸ªæ™ºèƒ½ä½“åä½œå®Œæˆå¤æ‚ä»»åŠ¡",
                requirements=["å¤šæ™ºèƒ½ä½“æ”¯æŒ", "åä½œæœºåˆ¶", "ä»»åŠ¡åˆ†é…", "é€šä¿¡åè®®"],
                recommended_frameworks=["crewai", "autogen", "metagpt"],
                priority_weights={
                    "multi_agent_support": 0.4,
                    "workflow_management": 0.3,
                    "memory_management": 0.2,
                    "error_handling": 0.1
                }
            ),
            "workflow_automation": UseCase(
                name="å·¥ä½œæµè‡ªåŠ¨åŒ–",
                description="è‡ªåŠ¨åŒ–å¤æ‚çš„å·¥ä½œæµç¨‹",
                requirements=["å·¥ä½œæµç®¡ç†", "çŠ¶æ€æœº", "æ¡ä»¶åˆ†æ”¯", "é”™è¯¯å¤„ç†"],
                recommended_frameworks=["langgraph", "langchain"],
                priority_weights={
                    "workflow_management": 0.4,
                    "error_handling": 0.25,
                    "async_support": 0.2,
                    "performance_score": 0.15
                }
            ),
            "enterprise_application": UseCase(
                name="ä¼ä¸šçº§åº”ç”¨",
                description="å¼€å‘ä¼ä¸šçº§AIåº”ç”¨",
                requirements=["ä¼ä¸šå°±ç»ª", "å®‰å…¨ç‰¹æ€§", "æ€§èƒ½", "å¯ç»´æŠ¤æ€§"],
                recommended_frameworks=["langchain", "crewai"],
                priority_weights={
                    "enterprise_readiness": 0.3,
                    "security_features": 0.25,
                    "performance_score": 0.2,
                    "documentation_quality": 0.15,
                    "error_handling": 0.1
                }
            ),
            "research_experiment": UseCase(
                name="ç ”ç©¶å®éªŒ",
                description="è¿›è¡ŒAIç ”ç©¶å®éªŒ",
                requirements=["çµæ´»æ€§", "å¯æ‰©å±•æ€§", "å¤šæ™ºèƒ½ä½“æ”¯æŒ", "æˆæœ¬æ•ˆç‡"],
                recommended_frameworks=["autogen", "camel", "metagpt"],
                priority_weights={
                    "multi_agent_support": 0.3,
                    "cost_efficiency": 0.25,
                    "feature_completeness": 0.25,
                    "learning_curve": 0.2
                }
            ),
            "rapid_prototyping": UseCase(
                name="å¿«é€ŸåŸå‹",
                description="å¿«é€Ÿæ„å»ºAIåº”ç”¨åŸå‹",
                requirements=["å­¦ä¹ æ›²çº¿", "å¼€å‘é€Ÿåº¦", "å·¥å…·ç”Ÿæ€", "æ–‡æ¡£è´¨é‡"],
                recommended_frameworks=["crewai", "langchain"],
                priority_weights={
                    "learning_curve": 0.35,
                    "tool_ecosystem": 0.25,
                    "documentation_quality": 0.25,
                    "community_activity": 0.15
                }
            )
        }
    
    def compare_frameworks(self, framework_names: List[str]) -> Dict[str, Any]:
        """å¯¹æ¯”æŒ‡å®šæ¡†æ¶"""
        logger.info(f"Comparing frameworks: {framework_names}")
        
        comparison_data = {}
        
        for framework_name in framework_names:
            if framework_name in self.frameworks:
                framework = self.frameworks[framework_name]
                comparison_data[framework_name] = {
                    "name": framework.name,
                    "github_stars": framework.github_stars,
                    "last_updated": framework.last_updated,
                    "documentation_quality": framework.documentation_quality,
                    "community_activity": framework.community_activity,
                    "learning_curve": framework.learning_curve.value,
                    "performance_score": framework.performance_score,
                    "feature_completeness": framework.feature_completeness,
                    "enterprise_readiness": framework.enterprise_readiness,
                    "security_features": framework.security_features,
                    "tool_ecosystem": framework.tool_ecosystem,
                    "multi_agent_support": framework.multi_agent_support,
                    "workflow_management": framework.workflow_management,
                    "memory_management": framework.memory_management,
                    "error_handling": framework.error_handling,
                    "async_support": framework.async_support,
                    "cost_efficiency": framework.cost_efficiency
                }
        
        return comparison_data
    
    def recommend_framework(self, use_case: str, requirements: Dict[str, float]) -> List[Tuple[str, float]]:
        """æ¨èæ¡†æ¶"""
        logger.info(f"Recommending framework for use case: {use_case}")
        
        if use_case not in self.use_cases:
            raise ValueError(f"Unknown use case: {use_case}")
        
        use_case_data = self.use_cases[use_case]
        
        # è®¡ç®—æ¯ä¸ªæ¡†æ¶çš„å¾—åˆ†
        framework_scores = {}
        
        for framework_name, framework in self.frameworks.items():
            score = 0.0
            
            for requirement, weight in requirements.items():
                if hasattr(framework, requirement):
                    score += getattr(framework, requirement) * weight
            
            framework_scores[framework_name] = score
        
        # æŒ‰å¾—åˆ†æ’åº
        sorted_frameworks = sorted(
            framework_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return sorted_frameworks
    
    def generate_comparison_report(self, framework_names: List[str]) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        logger.info(f"Generating comparison report for: {framework_names}")
        
        comparison_data = self.compare_frameworks(framework_names)
        
        report = f"""
# AIæ¡†æ¶å¯¹æ¯”åˆ†ææŠ¥å‘Š

## æ¦‚è§ˆ
æœ¬æŠ¥å‘Šå¯¹æ¯”åˆ†æäº†ä»¥ä¸‹AIæ¡†æ¶ï¼š
{', '.join([self.frameworks[name].name for name in framework_names])}

## è¯¦ç»†å¯¹æ¯”

### 1. åŸºç¡€ä¿¡æ¯
"""
        
        for framework_name, data in comparison_data.items():
            report += f"""
#### {data['name']}
- GitHub Stars: {data['github_stars']:,}
- æœ€åæ›´æ–°: {data['last_updated']}
- å­¦ä¹ æ›²çº¿: {data['learning_curve']}
"""
        
        report += """
### 2. æŠ€æœ¯æŒ‡æ ‡å¯¹æ¯”
"""
        
        # æŠ€æœ¯æŒ‡æ ‡å¯¹æ¯”è¡¨
        metrics = [
            "documentation_quality", "community_activity", "performance_score",
            "feature_completeness", "enterprise_readiness", "security_features",
            "tool_ecosystem", "multi_agent_support", "workflow_management",
            "memory_management", "error_handling", "async_support", "cost_efficiency"
        ]
        
        report += "| æŒ‡æ ‡ | " + " | ".join([data['name'] for data in comparison_data.values()]) + " |\n"
        report += "|------|" + "|".join(["------" for _ in comparison_data]) + "|\n"
        
        for metric in metrics:
            row = f"| {metric.replace('_', ' ').title()} |"
            for data in comparison_data.values():
                row += f" {data[metric]:.1f} |"
            report += row + "\n"
        
        report += """
### 3. ä¼˜åŠ¿åˆ†æ
"""
        
        for framework_name, data in comparison_data.items():
            report += f"""
#### {data['name']} ä¼˜åŠ¿
- æ–‡æ¡£è´¨é‡: {data['documentation_quality']}/10
- ç¤¾åŒºæ´»è·ƒåº¦: {data['community_activity']}/10
- å·¥å…·ç”Ÿæ€: {data['tool_ecosystem']}/10
- ä¼ä¸šå°±ç»ª: {data['enterprise_readiness']}/10
"""
        
        report += """
### 4. æ¨èåœºæ™¯
"""
        
        for use_case_name, use_case in self.use_cases.items():
            report += f"""
#### {use_case.name}
- æè¿°: {use_case.description}
- æ¨èæ¡†æ¶: {', '.join(use_case.recommended_frameworks)}
- å…³é”®éœ€æ±‚: {', '.join(use_case.requirements)}
"""
        
        report += f"""
### 5. æ€»ç»“å»ºè®®

1. **æ–°é¡¹ç›®å¼€å‘**: æ¨èä½¿ç”¨ LangChainï¼Œç”Ÿæ€æœ€ä¸°å¯Œ
2. **å¤šæ™ºèƒ½ä½“åä½œ**: æ¨èä½¿ç”¨ CrewAI æˆ– AutoGen
3. **å·¥ä½œæµç®¡ç†**: æ¨èä½¿ç”¨ LangGraph
4. **ä¼ä¸šçº§åº”ç”¨**: æ¨èä½¿ç”¨ LangChain + è‡ªå®šä¹‰æ‰©å±•
5. **ç ”ç©¶å®éªŒ**: æ¨èä½¿ç”¨ AutoGen æˆ– CAMEL

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report
    
    def benchmark_frameworks(self, framework_names: List[str], test_cases: List[str]) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•"""
        logger.info(f"Benchmarking frameworks: {framework_names}")
        
        benchmark_results = {}
        
        for framework_name in framework_names:
            if framework_name not in self.frameworks:
                continue
            
            framework = self.frameworks[framework_name]
            
            # æ¨¡æ‹ŸåŸºå‡†æµ‹è¯•
            start_time = time.time()
            
            # æ¨¡æ‹Ÿä¸åŒæµ‹è¯•ç”¨ä¾‹çš„æ‰§è¡Œæ—¶é—´
            test_results = {}
            for test_case in test_cases:
                # æ¨¡æ‹Ÿæµ‹è¯•æ‰§è¡Œæ—¶é—´
                execution_time = time.sleep(0.1) or 0.1
                test_results[test_case] = {
                    "execution_time": execution_time,
                    "success_rate": 0.95,
                    "memory_usage": 100 + framework.performance_score * 10
                }
            
            total_time = time.time() - start_time
            
            benchmark_results[framework_name] = {
                "framework_name": framework.name,
                "total_time": total_time,
                "test_results": test_results,
                "overall_score": framework.performance_score
            }
        
        return benchmark_results
    
    def analyze_trends(self) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯è¶‹åŠ¿"""
        logger.info("Analyzing AI framework trends")
        
        # åˆ†æGitHub Starsè¶‹åŠ¿
        stars_data = [(name, framework.github_stars) for name, framework in self.frameworks.items()]
        stars_data.sort(key=lambda x: x[1], reverse=True)
        
        # åˆ†æåŠŸèƒ½å®Œæ•´æ€§è¶‹åŠ¿
        completeness_data = [(name, framework.feature_completeness) for name, framework in self.frameworks.items()]
        completeness_data.sort(key=lambda x: x[1], reverse=True)
        
        # åˆ†æä¼ä¸šå°±ç»ªè¶‹åŠ¿
        enterprise_data = [(name, framework.enterprise_readiness) for name, framework in self.frameworks.items()]
        enterprise_data.sort(key=lambda x: x[1], reverse=True)
        
        trends = {
            "github_stars_ranking": stars_data,
            "feature_completeness_ranking": completeness_data,
            "enterprise_readiness_ranking": enterprise_data,
            "emerging_trends": [
                "å¤šæ™ºèƒ½ä½“åä½œæˆä¸ºä¸»æµ",
                "å·¥ä½œæµç®¡ç†æ—¥ç›Šé‡è¦",
                "ä¼ä¸šçº§ç‰¹æ€§éœ€æ±‚å¢é•¿",
                "å®‰å…¨æ§åˆ¶ä¸æ–­åŠ å¼º",
                "æ€§èƒ½ä¼˜åŒ–æŒç»­è¿›è¡Œ"
            ],
            "future_directions": [
                "æ™ºèƒ½åŒ–ç¨‹åº¦æå‡",
                "åä½œèƒ½åŠ›å¢å¼º",
                "å·¥å…·ç”Ÿæ€æ‰©å±•",
                "æ€§èƒ½æŒç»­ä¼˜åŒ–"
            ]
        }
        
        return trends
    
    def generate_selection_guide(self) -> str:
        """ç”Ÿæˆé€‰æ‹©æŒ‡å—"""
        logger.info("Generating framework selection guide")
        
        guide = """
# AIæ¡†æ¶é€‰æ‹©æŒ‡å—

## é€‰æ‹©æµç¨‹

### 1. æ˜ç¡®éœ€æ±‚
- ç¡®å®šé¡¹ç›®ç±»å‹å’Œè§„æ¨¡
- è¯†åˆ«å…³é”®åŠŸèƒ½éœ€æ±‚
- è¯„ä¼°å›¢é˜ŸæŠ€æœ¯èƒ½åŠ›
- è€ƒè™‘é¢„ç®—å’Œæ—¶é—´é™åˆ¶

### 2. è¯„ä¼°æ¡†æ¶
- å¯¹æ¯”æŠ€æœ¯æŒ‡æ ‡
- åˆ†æå­¦ä¹ æ›²çº¿
- æ£€æŸ¥ç¤¾åŒºæ”¯æŒ
- è¯„ä¼°ä¼ä¸šçº§ç‰¹æ€§

### 3. åšå‡ºå†³ç­–
- åŸºäºéœ€æ±‚æƒé‡è¯„åˆ†
- è€ƒè™‘é•¿æœŸç»´æŠ¤æˆæœ¬
- è¯„ä¼°æ‰©å±•æ€§éœ€æ±‚
- åˆ¶å®šè¿ç§»è®¡åˆ’

## å†³ç­–çŸ©é˜µ

| éœ€æ±‚ç±»å‹ | æ¨èæ¡†æ¶ | ç†ç”± |
|----------|----------|------|
| é€šç”¨AIåº”ç”¨ | LangChain | ç”Ÿæ€æœ€ä¸°å¯Œï¼Œå·¥å…·æœ€å¤š |
| å¤šæ™ºèƒ½ä½“åä½œ | CrewAI | ä¸“ä¸šçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ |
| å·¥ä½œæµç®¡ç† | LangGraph | å¼ºå¤§çš„çŠ¶æ€æœºæ”¯æŒ |
| ä¼ä¸šçº§åº”ç”¨ | LangChain | ä¼ä¸šçº§ç‰¹æ€§æœ€å®Œå–„ |
| ç ”ç©¶å®éªŒ | AutoGen | çµæ´»æ€§æœ€é«˜ |
| å¿«é€ŸåŸå‹ | CrewAI | å­¦ä¹ æ›²çº¿æœ€å¹³ç¼“ |

## æœ€ä½³å®è·µ

### 1. ä»ç®€å•å¼€å§‹
- é€‰æ‹©å­¦ä¹ æ›²çº¿å¹³ç¼“çš„æ¡†æ¶
- ä»åŸºç¡€åŠŸèƒ½å¼€å§‹æ„å»º
- é€æ­¥å¢åŠ å¤æ‚åº¦

### 2. å……åˆ†åˆ©ç”¨ç”Ÿæ€
- ä½¿ç”¨æ¡†æ¶æä¾›çš„å·¥å…·
- å‚ä¸ç¤¾åŒºè®¨è®º
- è´¡çŒ®å¼€æºä»£ç 

### 3. æ³¨é‡å¯ç»´æŠ¤æ€§
- ç¼–å†™æ¸…æ™°çš„ä»£ç 
- å®Œå–„çš„æ–‡æ¡£
- å®šæœŸæ›´æ–°ä¾èµ–

### 4. è€ƒè™‘æ€§èƒ½
- ä¼˜åŒ–å…³é”®è·¯å¾„
- ç›‘æ§èµ„æºä½¿ç”¨
- å®æ–½ç¼“å­˜ç­–ç•¥

### 5. å®‰å…¨ç¬¬ä¸€
- å®æ–½è¾“å…¥éªŒè¯
- æ§åˆ¶æƒé™èŒƒå›´
- è®°å½•æ“ä½œæ—¥å¿—

## å¸¸è§é™·é˜±

### 1. è¿‡åº¦å¤æ‚åŒ–
- é¿å…è¿‡æ—©ä¼˜åŒ–
- ä¿æŒç®€å•æœ‰æ•ˆ
- é€æ­¥å¢åŠ åŠŸèƒ½

### 2. å¿½è§†ç»´æŠ¤æˆæœ¬
- è€ƒè™‘é•¿æœŸç»´æŠ¤
- è¯„ä¼°æ›´æ–°é¢‘ç‡
- è§„åˆ’è¿ç§»è·¯å¾„

### 3. æŠ€æœ¯å€ºåŠ¡
- åŠæ—¶é‡æ„ä»£ç 
- ä¿æŒä»£ç è´¨é‡
- å®šæœŸæŠ€æœ¯å®¡æŸ¥

### 4. æ€§èƒ½é—®é¢˜
- ç›‘æ§å…³é”®æŒ‡æ ‡
- ä¼˜åŒ–ç“¶é¢ˆç‚¹
- å®æ–½æ€§èƒ½æµ‹è¯•

## æ€»ç»“

é€‰æ‹©åˆé€‚çš„AIæ¡†æ¶éœ€è¦ç»¼åˆè€ƒè™‘é¡¹ç›®éœ€æ±‚ã€å›¢é˜Ÿèƒ½åŠ›ã€æŠ€æœ¯ç‰¹æ€§å’Œé•¿æœŸç»´æŠ¤æˆæœ¬ã€‚
å»ºè®®ä»ç®€å•å¼€å§‹ï¼Œé€æ­¥å¤æ‚åŒ–ï¼Œå……åˆ†åˆ©ç”¨æ¡†æ¶ç”Ÿæ€ï¼Œæ³¨é‡ä»£ç è´¨é‡å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

---
*æŒ‡å—ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return guide

def run_comparison_analysis():
    """è¿è¡Œå¯¹æ¯”åˆ†æ"""
    print("ğŸ” AIæ¡†æ¶å¯¹æ¯”åˆ†æå·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºå¯¹æ¯”åˆ†æå™¨
    comparator = FrameworkComparison()
    
    # 1. åŸºç¡€å¯¹æ¯”
    print("\nğŸ“Š åŸºç¡€æ¡†æ¶å¯¹æ¯”")
    print("-" * 30)
    
    framework_names = ["langchain", "crewai", "autogen", "langgraph"]
    comparison_data = comparator.compare_frameworks(framework_names)
    
    for framework_name, data in comparison_data.items():
        print(f"{data['name']}: {data['github_stars']:,} stars, å­¦ä¹ æ›²çº¿: {data['learning_curve']}")
    
    # 2. ä½¿ç”¨åœºæ™¯æ¨è
    print("\nğŸ¯ ä½¿ç”¨åœºæ™¯æ¨è")
    print("-" * 30)
    
    use_cases = ["general_ai_app", "multi_agent_collaboration", "workflow_automation"]
    for use_case in use_cases:
        if use_case in comparator.use_cases:
            use_case_data = comparator.use_cases[use_case]
            recommendations = comparator.recommend_framework(use_case, use_case_data.priority_weights)
            print(f"{use_case_data.name}: {recommendations[0][0]} (å¾—åˆ†: {recommendations[0][1]:.2f})")
    
    # 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print("\nğŸ“„ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š")
    print("-" * 30)
    
    report = comparator.generate_comparison_report(framework_names)
    
    # ä¿å­˜æŠ¥å‘Š
    report_filename = f"framework_comparison_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_filename}")
    
    # 4. åŸºå‡†æµ‹è¯•
    print("\nâš¡ åŸºå‡†æµ‹è¯•")
    print("-" * 30)
    
    test_cases = ["basic_task", "complex_workflow", "multi_agent_collaboration"]
    benchmark_results = comparator.benchmark_frameworks(framework_names, test_cases)
    
    for framework_name, results in benchmark_results.items():
        print(f"{results['framework_name']}: æ€»ä½“å¾—åˆ† {results['overall_score']:.1f}")
    
    # 5. è¶‹åŠ¿åˆ†æ
    print("\nğŸ“ˆ è¶‹åŠ¿åˆ†æ")
    print("-" * 30)
    
    trends = comparator.analyze_trends()
    print("GitHub Starsæ’å:")
    for i, (name, stars) in enumerate(trends["github_stars_ranking"][:3], 1):
        print(f"{i}. {name}: {stars:,} stars")
    
    # 6. ç”Ÿæˆé€‰æ‹©æŒ‡å—
    print("\nğŸ“– ç”Ÿæˆé€‰æ‹©æŒ‡å—")
    print("-" * 30)
    
    guide = comparator.generate_selection_guide()
    
    # ä¿å­˜æŒ‡å—
    guide_filename = f"framework_selection_guide_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    with open(guide_filename, 'w', encoding='utf-8') as f:
        f.write(guide)
    
    print(f"é€‰æ‹©æŒ‡å—å·²ä¿å­˜åˆ°: {guide_filename}")
    
    print("\nâœ… å¯¹æ¯”åˆ†æå®Œæˆï¼")
    
    return {
        "comparison_data": comparison_data,
        "benchmark_results": benchmark_results,
        "trends": trends,
        "report_filename": report_filename,
        "guide_filename": guide_filename
    }

if __name__ == "__main__":
    run_comparison_analysis()
